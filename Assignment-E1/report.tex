\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}

\newcommand{\elbo}{\mathcal{L}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\p}{p}
\newcommand{\q}{q}

\title{E1 - Assignment}
\author{
Bruno Carchia \\
\texttt{carchia@kth.se}
\and
Riccardo Alfonso Cerrone \\
  \texttt{cerrone@kth.se}
}

\begin{document}
\maketitle

\begin{align*}
\mathbb{P}(X_2 = x_2 \big | X_4 = \text{thin}) &= \dfrac{\mathbb{P}( X_4 = \text{thin} \big | X_2 = x_2) \mathbb{P}(X_2 = x_2)}{\mathbb{P}(X_4 = \text{thin})}
\\
& \propto \mathbb{P}( X_4 = \text{thin} \big | X_2 = x_2) \mathbb{P}(X_2 = x_2)
\\ 
&= \mathbb{P}( X_4 = \text{thin} \big | X_2 = x_2) \sum_{x_1}\mathbb{P}(X_2 = x_2, X_1=x_1)
\\
&= \mathbb{P}( X_4 = \text{thin} \big | X_2 = x_2) \sum_{x_1}\mathbb{P}(X_2 = x_2 \big| X_1=x_1)\mathbb{P}(X_1=x_1)
\end{align*}

\newpage

\begin{align*}
\mathbb{P}(X_1=x_1 \big | X_3 = \text{medium}, X_4 = \text{thin}) &= \dfrac{\mathbb{P}(X_1=x_1, X_3 = \text{medium}, X_4 = \text{thin})}{\mathbb{P}(X_3 = \text{medium})\mathbb{P}(X_4 = \text{thin} \big | X_3 = \text{medium})}
\\
& \propto \mathbb{P}(X_1=x_1, X_3 = \text{medium}, X_4 = \text{thin})
\\
&= \mathbb{P}(X_1 = x_1) \textcolor{red}{\mathbb{P}(X_3= \text{medium} \big | X_1 = x_1)} \\
& \textcolor{blue}{\mathbb{P}(X_4 = \text{thin} \big | X_3 = \text{medium}, X_1 = x_1)}
\end{align*}

\begin{align*}
\textcolor{blue}{\mathbb{P}(X_4 = \text{T} \big | X_3 = \text{M}, X_1 = x_1)} &= \sum_{x_2} \mathbb{P}(X_4= \text{T}, X_2 = x_2 \big |X_3 = \text{M}, X_1 = x_1) 
\\
&= \sum_{x_2} \mathbb{P}(X_4= \text{T}, X_2 = x_2 \big |X_3 = \text{M}, X_1 = x_1) 
\end{align*}

\newpage

\section{CAVI}
\subsection{Question 1.3.12}
\subsubsection{Introduction}

\section{CAVI}
\subsection{Question 1.3.12}
\subsubsection{Introduction}

\subsubsection{Derivation of CAVI updates}

\begin{equation}
  \q(\mu) \sim \mathcal{N} (\mu_N, \lambda_N)
  \label{eq:q_mu}
\end{equation}

\begin{equation}
  \mu_N = \dfrac{\lambda_0 \mu_0 + \sum x_n}{\lambda_0 + N}
  \label{eq:mu_N}
\end{equation}

\begin{equation}
  \lambda_N = \E_{q(\tau)}[\tau](\lambda_0 + N)
  \label{eq:lambda_N}
\end{equation}


\begin{equation}
  \q(\tau) \sim \text{Gam} (a_N, b_N)
  \label{eq:q_tau}
\end{equation}


\begin{equation}
  a_N = a_0 + \dfrac{N+1}{2}
  \label{eq:a_N}
\end{equation}

\begin{equation}
  b_N = b_0 + \dfrac{1}{2}\E_{q(\mu)}\bigg[
\lambda_0 (\mu - \mu_0)^2 + \sum^N_{n = 1} (x_i - \mu)^2
\bigg] 
  \label{eq:b_N}
\end{equation}


\subsubsection{Derivation of the ELBO}

We now expand the Evidence Lower Bound:

\begin{align*}
\elbo 
&= \E_q \left[ \log \frac{\p(D, \mu, \tau)}{\q(\mu, \tau)} \right] \\
&= \E_q [\log \p(D, \mu, \tau)] - \E_q [\log \q (\mu, \tau)]\\
&= \textcolor{red}{\E_q [\log \p (\mu \mid \lambda_0, \mu_0, \tau)]}
+ \textcolor{blue}{\E_q [\log \p (\tau \mid a_0, b_0)]} 
+ \textcolor{olive}{\E_q [\log \p (D \mid \mu, \tau)]}
\\ &\quad- \textcolor{orange}{\E_q [\log \q (\mu)]} 
- \textcolor{magenta}{\E_q [\log \q (\tau)]}
\end{align*}

The following sections compute each coloured term separately.

\paragraph{Term 1: \( \E_q[\log p(\mu \mid \lambda_0, \mu_0, \tau)] \)}
We use the Normal prior over \(\mu\), whose log-density depends on 
\(\log\tau\) and on the quadratic term \((\mu-\mu_0)^2\). Substituting
the expressions and applying expectations using the moments of 
\(q(\mu)\) from \eqref{eq:q_mu}, we obtain:

\begin{align*}
\textcolor{red}{\E_q [\log \p (\mu \mid \lambda_0, \mu_0, \tau)]}
&= \E_q \bigg[ \frac{1}{2} \log \tau 
+ \frac{1}{2} \log \frac{\lambda_0}{2 \pi} 
- \frac{1}{2}\lambda_0 \tau (\mu - \mu_0)^2\bigg]
\\
&= \frac{1}{2} \log \frac{\lambda_0}{2 \pi} 
+ \frac{1}{2} \E_q [\log \tau ] 
- \frac{1}{2}\lambda_0 \E_q [\tau]\Big(\E_q[\mu^2]-2\mu_0\E_q[\mu]+\mu_0^2\Big)
\end{align*}

\paragraph{Term 2: \( \E_q[\log p(\tau \mid a_0, b_0)] \)}
Using the Gamma prior, whose log-density is linear in \(\log\tau\) and \(\tau\),
and substituting expectations using the parameters from \eqref{eq:a_N}–\eqref{eq:b_N}:

\begin{align*}
\textcolor{blue}{\E_q [\log \p (\tau \mid a_0, b_0)]} 
&= \E_q\big[a_0 \log b_0 - \log \Gamma(a_0) + (a_0 -1)\log \tau - b_0 \tau\big]\\
&= a_0 \log b_0 - \log \Gamma(a_0) 
+ (a_0 -1) \E_q[\log \tau] - b_0 \E_q[\tau]
\end{align*}


\paragraph{Term 3: \( \E_q[\log p(D \mid \mu, \tau)] \)}
The likelihood factorizes over the observations and contains terms depending on 
\((x_n - \mu)^2\). Using \(\E_q[(x_n-\mu)^2]\) computed from 
\eqref{eq:q_mu}, we obtain:

\begin{align*}
\textcolor{olive}{\E_q [\log \p (D \mid \mu, \tau)]} 
&= \E_q \bigg[ \log \bigg(\prod_{n = 1}^{N} \p (x_n \mid \mu, \tau)\bigg)\bigg]  
= \E_q \bigg[ \sum_{n = 1}^{N} \log \p (x_n \mid \mu, \tau)\bigg]
\\
&= \sum_{n = 1}^{N} \E_q \bigg[\log \bigg(\sqrt{\frac{\tau}{2 \pi}} 
\, e^{-\frac{\tau}{2}(x_n - \mu)^2}\bigg)\bigg] 
\\
&= \sum_{n = 1}^{N} \E_q \bigg[ \frac{1}{2}\log \tau 
- \frac{1}{2} \log (2 \pi) 
- \frac{\tau}{2}(x_n - \mu)^2 \bigg] 
\\
&= \frac{N}{2} \E_q [\log \tau] - \frac{N}{2} \log (2 \pi)
- \frac{1}{2} \E_q[\tau] 
\sum_{n = 1}^{N} \E_q\big[(x_n - \mu)^2\big]
\\
&= \frac{N}{2} \E_q [\log \tau] - \frac{N}{2} \log (2 \pi)
- \frac{1}{2} \E_q[\tau] 
\bigg(\sum_{n = 1}^{N} x_n^2 
- 2 \E_q[\mu]\sum_{n = 1}^{N} x_n 
+ N \E_q[\mu^2]\bigg)
\end{align*}


\paragraph{Term 4: \( \E_q[\log q(\mu)] \)} 
The entropy term of the Gaussian \(q(\mu)\) in \eqref{eq:q_mu} yields:

\begin{align*}
\textcolor{orange}{\E_q [\log \q (\mu)]} 
&= \E_q \bigg[\log \bigg(\sqrt{\frac{\lambda_N}{2 \pi}} 
\, e^{-\frac{\lambda_N}{2}(\mu - \mu_N)^2}\bigg)\bigg] 
\\
&= \E_q \bigg[ \frac{1}{2}\log \frac{\lambda_N}{2 \pi} 
- \frac{\lambda_N}{2}(\mu - \mu_N)^2 \bigg] 
\\
&= \frac{1}{2}\log \frac{\lambda_N}{2 \pi} 
- \frac{\lambda_N}{2} \E_q \big[(\mu - \mu_N)^2\big]
\\
&= \frac{1}{2}\log \frac{\lambda_N}{2 \pi} 
- \frac{\lambda_N}{2} \cdot \frac{1}{\lambda_N}
\\
&= \frac{1}{2}\bigg(\log \frac{\lambda_N}{2 \pi} - 1\bigg)
\end{align*}


\paragraph{Term 5: \( \E_q[\log q(\tau)] \)}
Finally, for the Gamma variational distribution in \eqref{eq:q_tau}:

\begin{align*}
\textcolor{magenta}{\E_q [\log \q (\tau)]} 
&= \E_q\big[a_N \log b_N - \log \Gamma(a_N) + (a_N -1)\log \tau - b_N \tau\big]\\
&= a_N \log b_N - \log \Gamma(a_N) 
+ (a_N -1) \E_q[\log \tau] - b_N \E_q[\tau]
\end{align*}

This completes the derivation of the ELBO, expressed entirely in terms of the 
variational parameters  
\(\mu_N\), \(\lambda_N\), \(a_N\), \(b_N\),  
which correspond directly to the CAVI updates given in 
\eqref{eq:q_mu}–\eqref{eq:b_N}, and \(\mu_0\), \(\lambda_0\), \(a_0\), \(b_0\).

\end{document}