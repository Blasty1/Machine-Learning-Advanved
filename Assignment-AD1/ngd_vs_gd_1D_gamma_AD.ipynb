{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d1cc2d036e890d",
   "metadata": {},
   "source": [
    "# Q 2.4.17. Standard Gradient Descent vs. Natural Gradient Descent\n",
    "In this notebbook, we compare standard gradient descent (GD) and natural gradient descent (NGD) for estimating the parameters of a 1D Gamma distribution from observed data. We will generate synthetic data from a known Gamma distribution and then attempt to recover its shape and scale parameters using both optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ecc5079214231",
   "metadata": {},
   "source": [
    "# 1. Configuration\n",
    "We define the true parameters of the Gamma distribution, the number of data points to generate, the learning rate, the number of epochs for optimization, and the initial guesses for the parameters. We also set a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4c1adc14b355c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1303199d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Gamma\n",
    "# ------------------------\n",
    "\n",
    "# True parameters of the Gamma distribution we want to discover\n",
    "ALPHA_TRUE = 3.0   # shape\n",
    "BETA_TRUE  = 2.0   # scale\n",
    "\n",
    "# Number of observed data points\n",
    "N_DATA = 1000\n",
    "\n",
    "# Optimization parameters\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 150\n",
    "\n",
    "# Initial \"wrong\" guess for our parameters (still positive)\n",
    "ALPHA_INIT = 0.5\n",
    "BETA_INIT  = 8.0\n",
    "\n",
    "# Fix random seed for reproducibility (optional)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d323836a2fe74",
   "metadata": {},
   "source": [
    "# 2. Data Generation\n",
    "We generate synthetic data from the true Gamma distribution using PyTorch's `Gamma` distribution class. We sample `N_DATA` points and print the sample mean and variance to verify the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1511b218a1abe489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 data points from Gamma(alpha=3.0, beta=2.0)\n",
      "Sample mean:     6.0692\n",
      "Sample variance: 11.9481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our parameterisation is shape–scale, but PyTorch's Gamma uses shape–rate.\n",
    "rate_true = 1.0 / BETA_TRUE\n",
    "dist_true = Gamma(concentration=torch.tensor(ALPHA_TRUE),\n",
    "                  rate=torch.tensor(rate_true))\n",
    "\n",
    "data = dist_true.sample((N_DATA,))\n",
    "\n",
    "print(f\"Generated {N_DATA} data points from Gamma(alpha={ALPHA_TRUE}, beta={BETA_TRUE})\")\n",
    "print(f\"Sample mean:     {data.mean().item():.4f}\")\n",
    "print(f\"Sample variance: {data.var().item():.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a54819ea48bcc",
   "metadata": {},
   "source": [
    "# 3. Loss function & parameter init\n",
    "We define the negative log-likelihood loss function for the Gamma distribution. We also initialize the parameters for both standard gradient descent and natural gradient descent, and set up history trackers to record the parameter values over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80885c47364f9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Loss function\n",
    "def gamma_nll(alpha, beta, data_points):\n",
    "    \"\"\"\n",
    "    ToDo:\n",
    "        Implement the average negative log-likelihood for Gamma distribution with shape=alpha and scale=beta.\n",
    "\n",
    "        Hints:\n",
    "        - Enforce positivity using clamp (e.g. min=1e-4).\n",
    "        - PyTorch's Gamma takes (concentration=alpha, rate=1/beta).\n",
    "        - Return the *mean* negative log-likelihood.\n",
    "\n",
    "    \"\"\"\n",
    "    alpha = torch.clamp(alpha, min=1e-4)\n",
    "    beta = torch.clamp(beta, min=1e-4)\n",
    "    \n",
    "    # Convert data_points to tensor if not already\n",
    "    data_tensor = torch.tensor(data_points) if not torch.is_tensor(data_points) else data_points\n",
    "    \n",
    "    empirical_mean_x = torch.tensor(data_tensor).mean()\n",
    "    empirical_mean_log_x = torch.log(data_tensor).mean()\n",
    "    nll = +alpha*torch.log(beta) + torch.lgamma(alpha) - (alpha-1)*empirical_mean_log_x + (1/beta)*empirical_mean_x\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34ac5ce61c6fb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters for Standard Gradient Descent (GD)\n",
    "alpha_gd = torch.tensor(ALPHA_INIT, requires_grad=True)\n",
    "beta_gd  = torch.tensor(BETA_INIT,  requires_grad=True)\n",
    "\n",
    "# Parameters for Natural Gradient Descent (NGD)\n",
    "alpha_ngd = torch.tensor(ALPHA_INIT, requires_grad=True)\n",
    "beta_ngd  = torch.tensor(BETA_INIT,  requires_grad=True)\n",
    "\n",
    "# History trackers\n",
    "history_gd  = []\n",
    "history_ngd = []\n",
    "history_loss_gd =[]\n",
    "history_loss_ngd = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b011aed1e45c60",
   "metadata": {},
   "source": [
    "# 4. Fisher Information inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b78f412721298b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_inverse(alpha, beta):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      Implement the inverse Fisher Information matrix F^{-1}(α, β)\n",
    "      for the Gamma(shape=α, scale=β) distribution.\n",
    "\n",
    "      Theory:\n",
    "        F(α, β) =\n",
    "            [ ψ1(α)        1/β      ]\n",
    "            [  1/β     α/β^2        ]\n",
    "\n",
    "        F^{-1}(α, β) =\n",
    "            1 / (α ψ1(α) - 1) *\n",
    "            [  α           -β             ]\n",
    "            [  -β       β^2 ψ1(α)         ]\n",
    "\n",
    "      Hints:\n",
    "      - Use torch.polygamma(1, alpha) for ψ1(α) (trigamma).\n",
    "      - Make sure to detach alpha, beta so F^{-1} is not part of the graph.\n",
    "    \"\"\"\n",
    "    alpha_detached = alpha.detach()\n",
    "    beta_detached = beta.detach()\n",
    "    \n",
    "    external_factor = 1/(alpha_detached* torch.polygamma(1, alpha_detached)-1)\n",
    "    inv11 = alpha_detached * external_factor\n",
    "    inv12 = -beta_detached * external_factor\n",
    "    inv22 = beta_detached**2 *  torch.polygamma(1, alpha_detached) * external_factor\n",
    "    return inv11, inv12, inv22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dbd73577cd34b",
   "metadata": {},
   "source": [
    "# 4. Optimization Loop\n",
    "We run the optimization loop for a specified number of epochs. In each epoch, we perform both standard gradient descent and natural gradient descent updates. We compute the gradients, build the Fisher Information Matrix for NGD, and update the parameters accordingly. We also log the parameter values and losses at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a82b37f0862057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing with LR=0.1 for 150 epochs...\n",
      "\n",
      "--- Epoch 1 ---\n",
      "  GD:  alpha=0.6516, beta=8.0032, Loss=3.1866\n",
      "  NGD: alpha=0.5340, beta=7.8695, Loss=3.1866\n",
      "\n",
      "--- Epoch 15 ---\n",
      "  GD:  alpha=1.0504, beta=7.9743, Loss=2.8331\n",
      "  NGD: alpha=1.2770, beta=4.3331, Loss=2.7370\n",
      "\n",
      "--- Epoch 30 ---\n",
      "  GD:  alpha=1.0853, beta=7.9160, Loss=2.8296\n",
      "  NGD: alpha=2.3102, beta=2.5556, Loss=2.5688\n",
      "\n",
      "--- Epoch 45 ---\n",
      "  GD:  alpha=1.0933, beta=7.8551, Loss=2.8270\n",
      "  NGD: alpha=2.8686, beta=2.1013, Loss=2.5450\n",
      "\n",
      "--- Epoch 60 ---\n",
      "  GD:  alpha=1.0989, beta=7.7937, Loss=2.8245\n",
      "  NGD: alpha=3.0273, beta=2.0018, Loss=2.5435\n",
      "\n",
      "--- Epoch 75 ---\n",
      "  GD:  alpha=1.1045, beta=7.7319, Loss=2.8219\n",
      "  NGD: alpha=3.0627, beta=1.9810, Loss=2.5434\n",
      "\n",
      "--- Epoch 90 ---\n",
      "  GD:  alpha=1.1101, beta=7.6698, Loss=2.8193\n",
      "  NGD: alpha=3.0701, beta=1.9768, Loss=2.5434\n",
      "\n",
      "--- Epoch 105 ---\n",
      "  GD:  alpha=1.1159, beta=7.6072, Loss=2.8167\n",
      "  NGD: alpha=3.0716, beta=1.9759, Loss=2.5434\n",
      "\n",
      "--- Epoch 120 ---\n",
      "  GD:  alpha=1.1217, beta=7.5444, Loss=2.8141\n",
      "  NGD: alpha=3.0719, beta=1.9757, Loss=2.5434\n",
      "\n",
      "--- Epoch 135 ---\n",
      "  GD:  alpha=1.1277, beta=7.4811, Loss=2.8114\n",
      "  NGD: alpha=3.0720, beta=1.9757, Loss=2.5434\n",
      "\n",
      "--- Epoch 150 ---\n",
      "  GD:  alpha=1.1339, beta=7.4175, Loss=2.8086\n",
      "  NGD: alpha=3.0720, beta=1.9756, Loss=2.5434\n",
      "\n",
      "Optimization finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/xt8_stpd459d9zhcnm21ykc40000gn/T/ipykernel_22582/1480512871.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  empirical_mean_x = torch.tensor(data_tensor).mean()\n"
     ]
    }
   ],
   "source": [
    "print(f\"Optimizing with LR={LEARNING_RATE} for {EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # ===== A. Standard Gradient Descent (GD) =====\n",
    "\n",
    "    if alpha_gd.grad is not None:\n",
    "        alpha_gd.grad.zero_()\n",
    "    if beta_gd.grad is not None:\n",
    "        beta_gd.grad.zero_()\n",
    "\n",
    "    loss_gd = gamma_nll(alpha_gd, beta_gd, data)\n",
    "    \n",
    "    loss_gd.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        alpha_gd -= LEARNING_RATE * alpha_gd.grad\n",
    "        beta_gd  -= LEARNING_RATE * beta_gd.grad\n",
    "\n",
    "        alpha_gd.clamp_(min=1e-4)\n",
    "        beta_gd.clamp_(min=1e-4)\n",
    "        \n",
    "    history_loss_gd.append(loss_gd.item())\n",
    "    history_gd.append((alpha_gd.item(), beta_gd.item()))\n",
    "\n",
    "    # ===== B. Natural Gradient Descent (NGD) =====\n",
    "\n",
    "    if alpha_ngd.grad is not None:\n",
    "        alpha_ngd.grad.zero_()\n",
    "    if beta_ngd.grad is not None:\n",
    "        beta_ngd.grad.zero_()\n",
    "\n",
    "    loss_ngd = gamma_nll(alpha_ngd, beta_ngd, data)\n",
    "    loss_ngd.backward()\n",
    "\n",
    "    g_alpha = alpha_ngd.grad\n",
    "    g_beta  = beta_ngd.grad\n",
    "\n",
    "    # ToDo : compute natural gradient using F^{-1}(α, β)\n",
    "    #  1) Get F^{-1} entries using fisher_inverse(...)\n",
    "    #  2) Compute:\n",
    "    #       - ng_alpha\n",
    "    #       - ng_beta\n",
    "\n",
    "    inv11, inv12, inv22 = fisher_inverse(alpha_ngd, beta_ngd)\n",
    "\n",
    "    ng_alpha =  inv11*g_alpha + inv12*g_beta\n",
    "    ng_beta  =  inv12*g_alpha + inv22*g_beta\n",
    "\n",
    "    with torch.no_grad():\n",
    "        alpha_ngd -= LEARNING_RATE * ng_alpha\n",
    "        beta_ngd  -= LEARNING_RATE * ng_beta\n",
    "\n",
    "        alpha_ngd.clamp_(min=1e-4)\n",
    "        beta_ngd.clamp_(min=1e-4)\n",
    "    \n",
    "    history_loss_ngd.append(loss_ngd.item())\n",
    "    history_ngd.append((alpha_ngd.item(), beta_ngd.item()))\n",
    "\n",
    "    if (epoch + 1) % 15 == 0 or epoch == 0:\n",
    "        print(f\"\\n--- Epoch {epoch + 1} ---\")\n",
    "        print(f\"  GD:  alpha={alpha_gd.item():.4f}, beta={beta_gd.item():.4f}, \"\n",
    "              f\"Loss={loss_gd.item():.4f}\")\n",
    "        print(f\"  NGD: alpha={alpha_ngd.item():.4f}, beta={beta_ngd.item():.4f}, \"\n",
    "              f\"Loss={loss_ngd.item():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nOptimization finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a5f27122c378f",
   "metadata": {},
   "source": [
    "# Q 2.4.18. Plotting Results\n",
    "To illustrate the difference between standard gradients and natural gradients, we print out the gradients computed in the first epoch for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51225312d5d3c319",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hist_gd_np  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(history_gd)\n\u001b[1;32m      2\u001b[0m hist_ngd_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(history_ngd)\n\u001b[0;32m----> 3\u001b[0m hist_loss_gd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(history_loss_gd)\n\u001b[1;32m      4\u001b[0m hist_loss_ngd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(history_loss_ngd)\n\u001b[1;32m      7\u001b[0m fig, (ax1, ax2,ax3) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m10\u001b[39m), sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:1226\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "\n",
    "hist_gd_np  = np.array(history_gd)\n",
    "hist_ngd_np = np.array(history_ngd)\n",
    "hist_loss_gd = np.array(history_loss_gd)\n",
    "hist_loss_ngd = np.array(history_loss_ngd)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "fig.suptitle(f\"Gamma: Standard Gradient vs. Natural Gradient \"\n",
    "             f\"(LR={LEARNING_RATE}, N={N_DATA})\", fontsize=16)\n",
    "\n",
    "# Plot 1: alpha (shape)\n",
    "ax1.plot(hist_gd_np[:, 0],  label=\"GD alpha\",  color='blue', linestyle='--')\n",
    "ax1.plot(hist_ngd_np[:, 0], label=\"NGD alpha\", color='red')\n",
    "ax1.axhline(ALPHA_TRUE, color='black', linestyle=':', label=f\"True alpha ({ALPHA_TRUE})\")\n",
    "ax1.set_ylabel(\"Shape parameter $\\\\alpha$\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: beta (scale)\n",
    "ax2.plot(hist_gd_np[:, 1],  label=\"GD beta\",  color='blue', linestyle='--')\n",
    "ax2.plot(hist_ngd_np[:, 1], label=\"NGD beta\", color='red')\n",
    "ax2.axhline(BETA_TRUE, color='black', linestyle=':', label=f\"True beta ({BETA_TRUE})\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Scale parameter $\\\\beta$\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Plot 3: Negative Log-Likelihood (NLL) Loss\n",
    "ax3.plot(hist_loss_gd,  label=\"GD NLL Loss\",  color='blue', linestyle='--')\n",
    "ax3.plot(hist_loss_ngd, label=\"NGD NLL Loss\", color='red')\n",
    "ax3.set_xlabel(\"Epoch\")\n",
    "ax3.set_ylabel(\"NLL Loss\")\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
