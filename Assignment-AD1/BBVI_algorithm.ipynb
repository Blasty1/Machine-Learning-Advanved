{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccbef42d4afb0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1196839f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969b4d0d27c1b47",
   "metadata": {},
   "source": [
    "# Black-Box Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95608b2cb5cd7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 data points.\n",
      "Data mean: 1.0740\n",
      "Data std dev: 1.2644\n",
      "Data shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "def generate_data(mu, tau, N):\n",
    "  x = torch.linspace(-10, 10, N)\n",
    "  # Insert your code here\n",
    "  sigma = 1 / torch.sqrt(torch.tensor(tau))   # precision Ï„ = 1/ÏƒÂ²\n",
    "  D = torch.normal(mu, sigma, size=(N,))\n",
    "\n",
    "  return D\n",
    "\n",
    "mu = 1\n",
    "tau = 0.5\n",
    "N= 100\n",
    "data= generate_data(mu, tau, N)\n",
    "\n",
    "print(f\"Generated {N} data points.\")\n",
    "print(f\"Data mean: {data.mean():.4f}\")\n",
    "print(f\"Data std dev: {data.std():.4f}\")\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2bd158ddd5dae3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data\n",
    "plt.hist(data.numpy(), bins=30, density=True, alpha=0.6, color='g')\n",
    "plt.title(\"Histogram of Generated Data\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb3003a17c7130",
   "metadata": {},
   "source": [
    "## ðŸ§® Part 1: Analytic Posterior Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c4f5a644efd51",
   "metadata": {},
   "source": [
    "The posterior $p(Î¼âˆ£D)=N(\\mu_N,Ïƒ^2_N)$ has parameters:\n",
    "$$\\frac{1}{\\sigma_N^2} = \\frac{1}{\\sigma_{\\text{prior}}^2} + \\frac{N}{\\sigma_{\\text{data}}^2}$$\n",
    "and\n",
    "$$\\mu_N = \\sigma_N^2 \\left( \\frac{\\mu_{\\text{prior}}}{\\sigma_{\\text{prior}}^2} + \\frac{\\sum_{i=1}^N x_i}{\\sigma_{\\text{data}}^2} \\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622bd92a570d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute Analytic Posterior ---\n",
    "\n",
    "# 1. Calculate Precisions (1 / variance)\n",
    "# Precision of the prior N(0, 1)\n",
    "prior_precision = 1.0 / PRIOR_VAR\n",
    "\n",
    "# Precision contributed by the N data points\n",
    "# Each point has variance DATA_VAR, so N points contribute N / DATA_VAR\n",
    "data_precision = N / DATA_VAR\n",
    "\n",
    "# 2. Posterior precision is the sum\n",
    "posterior_precision = prior_precision + data_precision\n",
    "\n",
    "# 3. Convert back to variance and standard deviation\n",
    "posterior_var = 1.0 / posterior_precision\n",
    "posterior_std = math.sqrt(posterior_var)\n",
    "\n",
    "# 4. Calculate Posterior Mean\n",
    "# It's a precision-weighted sum of means\n",
    "# (prior_precision * PRIOR_MEAN) + (data_precision * data.mean())\n",
    "# which is equivalent to:\n",
    "# (PRIOR_MEAN / PRIOR_VAR) + (data.sum() / DATA_VAR)\n",
    "\n",
    "posterior_mean = posterior_var * ( (PRIOR_MEAN / PRIOR_VAR) + (data.sum() / DATA_VAR) )\n",
    "\n",
    "print(\"--- Ground Truth Posterior ---\")\n",
    "print(f\"Posterior Mean (Î¼_N):   {posterior_mean.item():.4f}\")\n",
    "print(f\"Posterior Std Dev (Ïƒ_N): {posterior_std:.4f}\")\n",
    "print(f\"Posterior Var (Ïƒ_N^2):   {posterior_var:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e706353e531156e",
   "metadata": {},
   "source": [
    "## Part 2: BBVI Implementation - SGD vs Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b878404fe705",
   "metadata": {},
   "source": [
    "### Define model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3d3dab1ff6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_q_mu(m, s, K):\n",
    "    q_dist = dist.Normal(m, s)\n",
    "    return q_dist.sample((K,))\n",
    "\n",
    "def log_prior(mu):\n",
    "    log_prior_dist = dist.Normal(PRIOR_MEAN, math.sqrt(PRIOR_VAR))\n",
    "    return log_prior_dist.log_prob(mu)\n",
    "\n",
    "def log_likelihood(mu, data):\n",
    "    # We need to sum the log-likelihoods for all N data points\n",
    "    # We use broadcasting: mu is [K] or [1], data is [N]\n",
    "    # We want log_prob to be [K, N], then sum over N (dim=1)\n",
    "    return dist.Normal(mu.unsqueeze(1), math.sqrt(DATA_VAR)).log_prob(data).sum(dim=-1)\n",
    "\n",
    "def log_joint(mu, data):\n",
    "    return log_prior(mu) + log_likelihood(mu, data)\n",
    "\n",
    "def log_q_mu(mu, m, s):\n",
    "    q_dist = dist.Normal(m, s)\n",
    "    return q_dist.log_prob(mu)\n",
    "\n",
    "def score_function_mu(mu, m, s):\n",
    "    return (mu - m) / (s ** 2)\n",
    "\n",
    "def score_function_s(mu, m, s):\n",
    "    return -1. + ((mu - m) ** 2) / (s ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725283901b0fc0d",
   "metadata": {},
   "source": [
    "### Naive BBVI Implementation\n",
    "This implementation manually computes the score functions and constructs the surrogate loss as per the log-derivative trick.\n",
    "It follows the Naive BBVI algorithm, i.e., without Rao-Blackwellization and Control Variates, called Algorithm 1 in the BBVI paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1fb678a0cc61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbvi_naive_loss(m, log_s, data, K):\n",
    "    \"\"\"\n",
    "    Calculates the surrogate loss for BBVI.\n",
    "    Minimizing this loss is equivalent to maximizing the ELBO.\n",
    "    Following Algorithm 1 in the BBVI paper, we:\n",
    "    1. Sample K values from the variational distribution q(mu | lambda)\n",
    "    2. Evaluate it on log p(x, mu | sigma^2) - log q(mu) and score-function of q(mu).\n",
    "    3. Average over the samples to get a Monte Carlo estimate of the gradient of the ELBO.\n",
    "    \"\"\"\n",
    "    s = torch.exp(log_s)    # Optimize log_s than s directly since log_s is unconstrained\n",
    "\n",
    "    # 1. Sample mu ~ q(mu | lambda)\n",
    "    mu_samples = sample_q_mu(m, s, K)\n",
    "\n",
    "    # 2. Compute log q(mu)\n",
    "    log_q_vals = log_q_mu(mu_samples, m, s)\n",
    "\n",
    "    # 3. Compute log p(D, mu)\n",
    "    log_joint_vals = log_joint(mu_samples, data)\n",
    "\n",
    "    # 4. Compute the learning signal f(mu)\n",
    "    f = log_joint_vals - log_q_vals\n",
    "\n",
    "    # 5. Compute the score functions\n",
    "    score_m = score_function_mu(mu_samples, m, s)\n",
    "    score_s = score_function_s(mu_samples, m, s) # Actually score for log_s\n",
    "\n",
    "    # 6. Compute the surrogate loss\n",
    "    # We detach 'f' to treat it as a constant, as per the log-derivative trick\n",
    "    elbo = f.mean()    # MC estimate of ELBO\n",
    "    grad_m_ELBO_est = (score_m * f).mean()\n",
    "    grad_s_ELBO_est = (score_s * f).mean()\n",
    "\n",
    "    return grad_m_ELBO_est, grad_s_ELBO_est, elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20dc7c65e34b78",
   "metadata": {},
   "source": [
    "#### Run SGD Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608e729d4df281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational parameters\n",
    "m_sgd = torch.tensor(0.0, requires_grad=False)\n",
    "log_s_sgd = torch.tensor(0.0, requires_grad=False) # s=1.0\n",
    "\n",
    "# SGD parameters\n",
    "lr_sgd = 1e-7         # A small learning rate (Setting large will make it crash!)\n",
    "K = 10                # Number of MC samples per step\n",
    "T = 20000             # Number of iterations\n",
    "\n",
    "# History for plotting\n",
    "m_history_sgd = []\n",
    "s_history_sgd = []\n",
    "elbo_history_sgd = []\n",
    "\n",
    "print(f\"Running SGD with lr={lr_sgd}...\")\n",
    "pbar = tqdm(range(T))\n",
    "for t in pbar:\n",
    "    grad_m_ELBO_est, grad_s_ELBO_est, elbo = bbvi_naive_loss(m_sgd, log_s_sgd, data, K)\n",
    "\n",
    "    # SGD step\n",
    "    m_sgd += lr_sgd * grad_m_ELBO_est\n",
    "    log_s_sgd += lr_sgd * grad_s_ELBO_est\n",
    "\n",
    "    # Store results\n",
    "    m_history_sgd.append(m_sgd.item())\n",
    "    s_history_sgd.append(torch.exp(log_s_sgd).item())\n",
    "    elbo_history_sgd.append(elbo.item())\n",
    "\n",
    "    if (t + 1) % 500 == 0:\n",
    "        pbar.set_description(f\"ELBO: {elbo.item():.2f},\"\n",
    "                         f\" m: {m_sgd.item():.4f},\"\n",
    "                         f\" s: {torch.exp(log_s_sgd).item():.4f}\")\n",
    "\n",
    "print(\"SGD optimization finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0a1aee4e7cd2c",
   "metadata": {},
   "source": [
    "#### Visualize SGD results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eccf94917a9faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ELBO\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(elbo_history_sgd, label=\"ELBO\")\n",
    "plt.title(f\"SGD: ELBO (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1259b2d59817642",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot for m\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(m_history_sgd, label=\"SGD Estimate\")\n",
    "plt.axhline(posterior_mean.item(), color='r', linestyle='--', label=\"Ground Truth $\\mu_N$\")\n",
    "plt.title(f\"SGD: Convergence of Mean (m) (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for s\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(s_history_sgd, label=\"SGD Estimate\")\n",
    "plt.axhline(posterior_std, color='r', linestyle='--', label=\"Ground Truth $\\sigma_N$\")\n",
    "plt.title(f\"SGD: Convergence of Std Dev (s) (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56dbfed93f0793",
   "metadata": {},
   "source": [
    "### Naive BBVI loss using Automatic Differentiation\n",
    "The same as above, but we leverage PyTorch's autograd to compute gradients automatically.\n",
    "Note that we do not need to compute the score functions manually here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c2d981463d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbvi_naive_loss_auto_diff(m, log_s, data, K):\n",
    "    \"\"\"\n",
    "    Calculates the surrogate loss for BBVI.\n",
    "    Minimizing this loss is equivalent to maximizing the ELBO.\n",
    "    Here we sample K values from the variational distribution q(mu | lambda), evaluate it on both the log joint and log variational and average over the samples to get a Monte Carlo estimate of the loss.\n",
    "    \"\"\"\n",
    "    s = torch.exp(log_s)\n",
    "\n",
    "    # 1. Sample mu ~ q(mu | lambda)\n",
    "    mu_samples = sample_q_mu(m, s, K)\n",
    "\n",
    "    # 2. Compute log q(mu)\n",
    "    log_q_vals = log_q_mu(mu_samples, m, s)\n",
    "\n",
    "    # 3. Compute log p(D, mu)\n",
    "    log_joint_vals = log_joint(mu_samples, data)\n",
    "\n",
    "    # 4. Compute the learning signal f(mu)\n",
    "    f = log_joint_vals - log_q_vals\n",
    "\n",
    "    # 5. Compute the surrogate loss\n",
    "    # We detach 'f' to treat it as a constant, as per the log-derivative trick\n",
    "    elbo = f.detach().mean()    # MC estimate of ELBO\n",
    "\n",
    "    loss = -(log_q_vals * f.detach()).mean()\n",
    "\n",
    "    return loss, elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14df97bd36e0a13",
   "metadata": {},
   "source": [
    "*#### Optimization with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c623b4800464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational parameters\n",
    "m_sgd = torch.tensor(0.0, requires_grad=True)\n",
    "log_s_sgd = torch.tensor(0.0, requires_grad=True) # s=1.0\n",
    "\n",
    "# SGD parameters\n",
    "lr_sgd = 1e-5      # A small learning rate (Setting large will make it crash!)\n",
    "K = 10                # Number of MC samples per step\n",
    "T = 20000             # Number of iterations\n",
    "\n",
    "optimizer_sgd = optim.SGD([m_sgd, log_s_sgd], lr=lr_sgd)\n",
    "\n",
    "# History for plotting\n",
    "m_history_sgd = []\n",
    "s_history_sgd = []\n",
    "elbo_history_sgd = []\n",
    "\n",
    "print(f\"Running SGD with lr={lr_sgd}...\")\n",
    "pbar = tqdm(range(T))\n",
    "for t in pbar:\n",
    "    optimizer_sgd.zero_grad()\n",
    "    loss, elbo = bbvi_naive_loss_auto_diff(m_sgd, log_s_sgd, data, K)\n",
    "    loss.backward()\n",
    "    optimizer_sgd.step()\n",
    "\n",
    "    # Store results\n",
    "    m_history_sgd.append(m_sgd.item())\n",
    "    s_history_sgd.append(torch.exp(log_s_sgd).item())\n",
    "    elbo_history_sgd.append(elbo.item())\n",
    "\n",
    "    if (t + 1) % 500 == 0:\n",
    "        pbar.set_description(f\"ELBO: {elbo.item():.2f},\"\n",
    "                         f\" m: {m_sgd.item():.4f},\"\n",
    "                         f\" s: {torch.exp(log_s_sgd).item():.4f}\")\n",
    "\n",
    "print(\"SGD optimization finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043b3ca8a0692f1",
   "metadata": {},
   "source": [
    "#### Visualize SGD auto-diff results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223fd4db64d64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ELBO\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(elbo_history_sgd, label=\"ELBO\")\n",
    "plt.title(f\"SGD: ELBO (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33affc003c57ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot for m\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(m_history_sgd, label=\"SGD Estimate\")\n",
    "plt.axhline(posterior_mean.item(), color='r', linestyle='--', label=\"Ground Truth $\\mu_N$\")\n",
    "plt.title(f\"SGD: Convergence of Mean (m) (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for s\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(s_history_sgd, label=\"SGD Estimate\")\n",
    "plt.axhline(posterior_std, color='r', linestyle='--', label=\"Ground Truth $\\sigma_N$\")\n",
    "plt.title(f\"SGD: Convergence of Std Dev (s) (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03711b011c32e6e",
   "metadata": {},
   "source": [
    "### Adagrad Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfe64723c2a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variational parameters\n",
    "m_adagrad = torch.tensor(0.0, requires_grad=True)\n",
    "log_s_adagrad = torch.tensor(0.0, requires_grad=True) # s=1.0\n",
    "\n",
    "# Adagrad parameters\n",
    "lr_adagrad = 0.1     # A MUCH larger learning rate!\n",
    "K = 10               # Same number of MC samples\n",
    "T = 20000           # Same number of iterations\n",
    "\n",
    "optimizer_adagrad = optim.Adagrad([m_adagrad, log_s_adagrad], lr=lr_adagrad)\n",
    "\n",
    "# History for plotting\n",
    "m_history_adagrad = []\n",
    "s_history_adagrad = []\n",
    "elbo_history_adagrad = []\n",
    "\n",
    "# History for Q3\n",
    "eff_lr_m_history = []\n",
    "eff_lr_s_history = []\n",
    "\n",
    "print(f\"Running Adagrad with lr={lr_adagrad}...\")\n",
    "pbar = tqdm(range(T))\n",
    "for t in pbar:\n",
    "    optimizer_adagrad.zero_grad()\n",
    "    loss, elbo = bbvi_naive_loss_auto_diff(m_adagrad, log_s_adagrad, data, K)\n",
    "    loss.backward()\n",
    "    optimizer_adagrad.step()\n",
    "\n",
    "    # Store results\n",
    "    m_history_adagrad.append(m_adagrad.item())\n",
    "    s_history_adagrad.append(torch.exp(log_s_adagrad).item())\n",
    "    elbo_history_adagrad.append(elbo.item())\n",
    "\n",
    "    # --- For Q3: Store effective learning rates ---\n",
    "    # Access the optimizer's internal state 'sum' (which is G)\n",
    "    # Add optimizer's epsilon (default 1e-10) for numerical stability\n",
    "    g_m_sq_sum = optimizer_adagrad.state[m_adagrad]['sum'].item()\n",
    "    g_s_sq_sum = optimizer_adagrad.state[log_s_adagrad]['sum'].item()\n",
    "\n",
    "    eff_lr_m = lr_adagrad / (1e-10 + math.sqrt(g_m_sq_sum))\n",
    "    eff_lr_s = lr_adagrad / (1e-10 + math.sqrt(g_s_sq_sum))\n",
    "\n",
    "    eff_lr_m_history.append(eff_lr_m)\n",
    "    eff_lr_s_history.append(eff_lr_s)\n",
    "    # ---\n",
    "\n",
    "    if (t + 1) % 500 == 0:\n",
    "        pbar.set_description(f\"ELBO: {elbo.item():.2f},\"\n",
    "                            f\" m: {m_adagrad.item():.4f},\"\n",
    "                            f\" s: {torch.exp(log_s_adagrad).item():.4f}\")\n",
    "\n",
    "print(\"Adagrad optimization finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bc9b11f2f0827",
   "metadata": {},
   "source": [
    "#### Visualize Adagrad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c98acd3653ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ELBO\n",
    "print(f\"Final ELBO (Adagrad): {elbo_history_adagrad[-1]:.4f}\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(elbo_history_adagrad, label=\"ELBO\")\n",
    "plt.title(f\"Adagrad: ELBO (lr={lr_adagrad})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868619978ef1a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot for m\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(m_history_adagrad, label=\"Adagrad Estimate\")\n",
    "plt.axhline(posterior_mean.item(), color='r', linestyle='--', label=\"Ground Truth $\\mu_N$\")\n",
    "plt.title(f\"Adagrad: Convergence of Mean (m) (lr={lr_adagrad})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for s\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(s_history_adagrad, label=\"Adagrad Estimate\")\n",
    "plt.axhline(posterior_std, color='r', linestyle='--', label=\"Ground Truth $\\sigma_N$\")\n",
    "plt.title(f\"Adagrad: Convergence of Std Dev (s) (lr={lr_adagrad})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae19a810dc4cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Effective Learning Rates ---\n",
    "iterations_to_plot = slice(0, 100)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(eff_lr_m_history[iterations_to_plot], label=\"Effective LR for m\")\n",
    "plt.plot(eff_lr_s_history[iterations_to_plot], label=\"Effective LR for $\\log s$\")\n",
    "plt.title(\"Adagrad's Effective Learning Rate Over Time\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Effective Learning Rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bc5d698bdd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce2b9ee64242837e",
   "metadata": {},
   "source": [
    "## Part 3: Control Variates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4ddb8af9792fa",
   "metadata": {},
   "source": [
    "### Define BBVI loss with control variate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9efc7e9d47e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbvi_loss_with_control_variate(m, log_s, data, K, optimizer):\n",
    "    \"\"\"\n",
    "    Calculates the BBVI loss with control variates.\n",
    "    Following the from Algorithm 2 in the BBVI paper, we:\n",
    "    1. Sample K values from the variational distribution q(mu | lambda)\n",
    "    2. Evaluate it on log p(x, mu | sigma^2) - log q(mu) to compute the learning signal f\n",
    "    3. Compute the score functions analytically\n",
    "    4. Compute optimal baselines for variance reduction\n",
    "    5. Compute the gradient estimates using the control variates\n",
    "    6. Apply gradients manually using the provided optimizer\n",
    "    \"\"\"\n",
    "    s = torch.exp(log_s)\n",
    "    q_dist = dist.Normal(m, s)\n",
    "    mu_samples = q_dist.sample((K,))\n",
    "\n",
    "    # 1. Compute f (Learning Signal)\n",
    "    log_q = q_dist.log_prob(mu_samples)\n",
    "    log_joint_val = log_joint(mu_samples, data)\n",
    "    f = (log_joint_val - log_q).detach()\n",
    "    elbo = (log_joint_val - log_q).mean().item()  # MC estimate of ELBO\n",
    "\n",
    "    # 2. Compute Scores (Analytic)\n",
    "    score_m = (mu_samples - m) / s**2\n",
    "    score_s = -1 + ((mu_samples - m)**2 / s**2) # Score for log_s\n",
    "\n",
    "    # 3. Compute Optimal Baselines\n",
    "    a_m = (f * score_m**2).mean() / (score_m**2).mean()\n",
    "    a_s = (f * score_s**2).mean() / (score_s**2).mean()\n",
    "\n",
    "    # 4. Compute Gradients (The \"Black Box\" Gradient Estimator)\n",
    "    grad_m = (score_m * (f - a_m)).mean()\n",
    "    grad_s = (score_s * (f - a_s)).mean()\n",
    "\n",
    "    # 5. Apply Gradients Manually\n",
    "    # We need to negate them because we are maximizing, but optimizers minimize\n",
    "    optimizer.zero_grad()\n",
    "    m.grad = -grad_m\n",
    "    log_s.grad = -grad_s\n",
    "    optimizer.step()\n",
    "\n",
    "    return a_m.item(), a_s.item(), elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ebae85e291b8c",
   "metadata": {},
   "source": [
    "### Run SGD with Control Variate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61561e3ed6713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SGD and AdaGrad with Control Variate ---\n",
    "\n",
    "# Reset parameters\n",
    "m_cv = torch.tensor(0.0, requires_grad=True)\n",
    "log_s_cv = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Use a learning rate that failed/struggled in Part 2\n",
    "# (You can try 0.001 or even 0.01)\n",
    "lr_sgd = 0.001\n",
    "optimizer_cv = optim.SGD([m_cv, log_s_cv], lr=lr_sgd)\n",
    "#lr_adagrad = 1.0\n",
    "#optimizer_cv = optim.Adam([m_cv, log_s_cv], lr=lr_sgd)\n",
    "T = 2000\n",
    "\n",
    "m_history_cv = []\n",
    "s_history_cv = []\n",
    "elbo_history_cv = []\n",
    "\n",
    "print(f\"Running SGD with Control Variate (lr={lr_sgd})...\")\n",
    "pbar = tqdm(range(T))\n",
    "optimizer_cv.zero_grad()\n",
    "for t in pbar:\n",
    "\n",
    "    # Calculate loss using the *current* baseline\n",
    "    loss, current_batch_mean, elbo = bbvi_loss_with_control_variate(m_cv, log_s_cv, data, K=10, optimizer=optimizer_cv)\n",
    "\n",
    "    m_history_cv.append(m_cv.item())\n",
    "    s_history_cv.append(torch.exp(log_s_cv).item())\n",
    "    elbo_history_cv.append(elbo)\n",
    "\n",
    "    if (t + 1) % 500 == 0:\n",
    "        pbar.set_description(f\"ELBO: {elbo:.2f},\"\n",
    "                             f\"m: {m_cv.item():.4f},\"\n",
    "                            f\" s: {torch.exp(log_s_cv).item():.4f}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db92f0d2293636",
   "metadata": {},
   "source": [
    "### Plot Control Variate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438e1473e5ee23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ELBO\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(elbo_history_cv, label=\"ELBO\")\n",
    "plt.title(f\"SGD + Control Variate: ELBO (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab63a607c186f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "# Plot for m\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(m_history_cv, label=\"SGD + CV Estimate\")\n",
    "plt.axhline(posterior_mean.item(), color='r', linestyle='--', label=\"Ground Truth $\\mu_N$\")\n",
    "plt.title(f\"SGD + Control Variate: Convergence of Mean (m) (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for s\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(s_history_cv, label=\"SGD + CV Estimate\")\n",
    "plt.axhline(posterior_std, color='r', linestyle='--', label=\"Ground Truth $\\sigma_N$\")\n",
    "plt.title(f\"SGD + Control Variate: Convergence of Std Dev (s) (lr={lr_sgd})\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97debe749b95b569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
