\documentclass[a4paper]{scrartcl}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}


\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    showstringspaces=false,
    frame=single,
    backgroundcolor=\color{gray!10},
    literate=
        {τ}{{$\tau$}}1
        {σ}{{$\sigma$}}1
        {²}{{$^2$}}1
        {θ}{{$\theta$}}1
        {α}{{$\alpha$}}1
        {β}{{$\beta$}}1
        {λ}{{$\lambda$}}1
        {∇}{{$\nabla$}}1
        {≈}{{$\approx$}}1
        {Σ}{{$\Sigma$}}1
        {μ}{{$\mu$}}1
        {π}{{$\pi$}}1
        {ε}{{$\varepsilon$}}1
        {∈}{{$\in$}}1
        {≤}{{$\leq$}}1
        {≥}{{$\geq$}}1
        {×}{{$\times$}}1
        {∂}{{$\partial$}}1
        {∞}{{$\infty$}}1
        {ψ}{{$\psi$}}1
}

\newcommand{\elbo}{\mathcal{L}}
\newcommand{\E}{\mathbb{E}}

\title{Assignment 1AD, 2025}
\subtitle{DD2434 Machine Learning, Advanced Course}
\author{
Bruno Carchia \\
\texttt{carchia@kth.se}
}

\begin{document}
\maketitle

\section*{D Section}
\subsection*{D.1}
\subsubsection*{Question 1.1.1}
Starting from the definition of the KL divergence $\text{KL}(q(Z) || p(Z|X) )$ , we want to rewrite it and identify the quantity referred as the Evidence Lower Bound ( ELBO ).

\begin{align*}
\text{KL}(q(z) || p(z|x)) &= E_q \left[\log \frac{q(z)}{p(z|x)}\right] \\
&= \int q(z) \log \frac{q(z)}{p(z|x)} dz\\
&= \int q(z) \log \frac{q(z)}{\frac{p(z,x)}{p(x)}} dz\\
&= \int q(z) \log \frac{q(z)}{p(z,x)} dz + \int q(z) \log p(x) dz\\
&= E_q \left[ \log \frac{q(z)}{p(z,x)} \right] + \log p(x)
\end{align*}
Knowing that the ELBO is defined as $\mathcal{L} = \text{KL}(p(x,z) | q(z))$
\[
\text{KL}(q(z) || p(z|x)) = - \mathcal{L} + \log p(x)
\]
\[
\log p(x) = \mathcal{L} + \text{KL}(q(z) || p(z|x))
\]
The ELBO is a lower bound because the KL divergence is always non-negative. Maximizing the ELBO effectively maximizes the lower bound on $\log p(x)$ and minimizes the KL divergence.

\subsubsection*{Question 1.1.2}
\begin{itemize}
    \item A more expressive variational family can better approximate the true posterior, reducing $\text{KL}(q(z) || p(z|x))$ and increasing the ELBO ( a fully factorized mean field distribution produces a looser ELBO because it ignores dependencies between latent variables)
    \item A more expressive variational family can capture closely dependencies between latent variables, yielding $q(z)$ closer to the true posteriori $p(z|x)$ ( a fully factorized mean field distribution produces a less accurate posterior approximation )
\end{itemize}

\subsection*{D.2}
\subsubsection*{Question 1.1.3}
Considering a mean field assumption on our variation distribution $q(Z_1,Z_2,Z_3) = q(Z_1)q(Z_2)q(Z_3)$, we want to prove that $\log q_1^*(Z_1) = E_{-Z_1}\left[ \log p(X,Z) \right]$
\begin{align*}
    \mathcal{L} &= \int \prod_{i}^{3} q(z_i) \log \left( \frac{p(x,z)}{\prod_{l}^{3} q(z_l)} \right) dz\\
    &= \int \prod_{i}^{3} q(z_i) \left[ \log \left( p(x,z) \right) - \sum_{l}^{3} \log q(z_l)  \right] dz\\
    &= \int \prod_{i}^{3} q(z_i) \log  p(x,z) dz - \int \prod_{i}^{3} q(z_i) \sum_{l}^{3} \log q(z_l) dz
\end{align*}
We work individually on these two terms:
\begin{itemize}
    \item First term can be decomposed in an outer component that depends on $z_j$ and an the inner component that depends on all the other variables $z_{-j}$
    \[
    \int \prod_{i}^{3} q(z_i) \log  p(x,z) dz = \int_{z_j} q(z_j) \left[ \int_{z_{-j}} \prod_{i \ne j} q(z_i) \log p(x,z)  dz_{-j}\right] dz_j
    \]
    We define $\log \tilde{p}(x,z_j) = E_{\prod_{l \ne j} q(z_l)}[\log p(x,z)]$, the first term becomes
    \[
    \int_{z_j} q(z_j) \log \tilde{p}(x,z_j) dz_j
    \]
    \item Second term can be rewritten in the following way
    \[
    - \int \prod_{i}^{3} q(z_i) \sum_{l}^{3} \log q(z_l) dz = - \int_z  \sum_{l}^{3} \log q(z_l) \prod_{i}^{3} q(z_i) dz 
    \]
    After switching the sum and the integral, we can decompose the term in an outer (depends on all the variables excepting for $z_l$) and inner component ( depends just on $z_l$)
    \begin{align*}
    &= - \sum_{l}^{3} \int_z \log q(z_l) \prod_{i}^{3} q(z_i) dz\\
    &= - \sum_l^3 \int_{z_{-l}} \prod_{i \ne l} q(z_i) \left[ \int_{z_{l}} q(z_l) \log q(z_l) dz_l \right] dz_{-l}\\
    \end{align*}
    The inner component does not depend on terms related to $z_{-l}$ , the external integral gives 1 and we got as final result
    \begin{align*}
    &= - \sum_l^3  \int_{z_{l}} q(z_l) \log q(z_l) dz_l 
    \end{align*}
\end{itemize}
The ELBO is
\[
 \mathcal{L} =  \int_{z_j} q(z_j) \log \tilde{p}(x,z_j) dz_j - \sum_l^3  \int_{z_{l}} q(z_l) \log q(z_l) dz_l 
\]
But our objective is to maximize the ELBO by iteratively optimizing a single variational factor $q(z_j)$ while holding all other factors $q(z_{-j})$ constant. In other words we treat as constant all the terms that do not depend on ${z_j}$
\begin{align*}
\mathcal{L} &=  \int_{z_j} q(z_j) \log \tilde{p}(x,z_j) dz_j - \int_{z_{j}} q(z_j) \log q(z_j) dz_j + \text{const}\\
&= E_q(z_j)\left[ \frac{\log \tilde{p}(x,z_j)}{q(z_j)} \right]\\
&= - \text{KL}(\tilde{p}(x,z_j) || q(z_j))
\end{align*}
Maxiziming the ELBO wrt a single variational factor $q(z_j)$ is equivalent to minimizing $\text{KL}(\tilde{p}(x,z_j) || q(z_j))$: the minimum occurs when $q(z_j)* = \tilde{p}(x,z_j)$
\[
\log \tilde{p}(x,z_j) = \log q(z_j)* = E_{\prod_{l \ne j} q(z_l)}[\log p(x,z)]
\]
In this case $j=1$

\subsection*{D.3}
We will analyze the model with Normal-likelihood and NormalGamma prior of 1E.3. instead of using the Coordinate Ascent Variational Inference (CAVI) algorithm, we will employ Black-Box Variational Inference (BBVI). This BBVI will utilize the REINFORCE gradient estimator (in its basic, high-variance form) to infer the variational distributions $q(\mu)$ and $q(\tau)$ under the mean-field assumption $q(\mu, \tau) = q(\mu) q(\tau)$
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/D3-Normal-NormalGamma-model.png}
    \caption{Bayesian network of the Normal-NormalGamma model}
    \label{fig:Normal-NormalGammaModel}
\end{figure}
\subsubsection*{Question 1.2.4}
We provide the final expressisions for
\begin{itemize}
    \item Log Likelihood $\log P(D | \mu , \tau )$
\begin{align*}
    \log P(D | \mu , \tau ) &= \log \left(\prod_{n=1}^N f_{\tau,\mu}(x_n) \right)\\
    &= \sum_{n=1}^{N} \log \left( \sqrt{\frac{\tau}{2 \pi}} e^{- \frac{\tau}{2} (x_n-\mu)^2} \right)\\
    &= \sum_{n=1}^{N} \left[ 0.5 \log \tau -0.5 \log 2\pi - 0.5\tau (x_n - \mu)^2 \right]
\end{align*}
    \item Log Prior $\log P(\tau, \mu)$
\begin{align*}
    \log P(\tau, \mu) &= \log P(\tau) P(\mu | \tau)\\
    &= \log \left[ \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \sqrt{\frac{\lambda_0 \tau}{2\pi}} \tau^{\alpha_0 - 1} e^{-\beta_0 \tau} e^{-\frac{\lambda_0 \tau}{2} (\mu - \mu_0)^2} \right]\\
    &= \alpha_0 \log \beta_0 - \log \Gamma(\alpha_0) + 0.5 \log \lambda_0 \tau - 0.5 \log 2\pi + (\alpha_0 -1) \log \tau - \beta_0 \tau -\frac{\lambda_0 \tau}{2} (\mu - \mu_0)^2
\end{align*}
    \item Log Variational Distribution $q(z[s] | \lambda )$
\[
    \log q(\mu, \tau | \mu_N , \lambda_N, \alpha_N , \beta_N) = \log q(\mu | \mu_N, \lambda_N) + \log q(\tau | \alpha_N , \beta_N)
\]
Knowing that $q(\tau) \sim \text{Gamma}(\alpha_N, \beta_N)$ and $q(\mu) \sim \text{Normal}(\mu_N, \lambda_N^{-1})$
\[
    \left[ 0.5 \log \frac{\lambda_N}{2 \pi}  - \frac{\lambda_N (\mu - \mu_N)^2}{2}\right]
    +
    \left[ \alpha_N \log \beta_N +(\alpha_N -1)\log \tau -\log \Gamma(\alpha_N) - \beta_N \tau  \right]
\]
    \item Score function $\nabla_\lambda \log q(z[s] | \lambda)$
\[
    \nabla_{\mu_N , \lambda_N, \alpha_N , \beta_N} \log q(\mu, \tau | \mu_N , \lambda_N, \alpha_N , \beta_N) = \nabla_{\mu_N , \lambda_N} \log q(\mu | \mu_N , \lambda_N) + \nabla_{\alpha_N , \beta_N} \log q( \tau |\alpha_N , \beta_N)
\]
We compute each term individually
    \begin{itemize}
        \item $\nabla_{\mu_N , \lambda_N} \log q(\mu | \mu_N , \lambda_N)$
        \begin{itemize}
            \item $\nabla_{\mu_N} \log q(\mu | \mu_N , \lambda_N) = \lambda_N(\mu-\mu_N)$
            \item $\nabla_{\lambda_N} \log q(\mu | \mu_N , \lambda_N) = +0.5 \frac{\frac{1}{2\pi}}{\frac{\lambda_N}{2\pi}} - 0.5 (\mu-\mu_N)^2 = \frac{1}{2 \lambda_N} - \frac{(\mu-\mu_N)^2}{2}$
        \end{itemize}    
        \item $\nabla_{\alpha_N , \beta_N} \log q( \tau |\alpha_N , \beta_N)$
        \begin{itemize}
            \item $\nabla_{\alpha_N} \log q( \tau |\alpha_N , \beta_N) = \log \beta_N + \log \tau - \psi(\lambda_N)$ , where  $\psi(x) = \frac{d \ln \Gamma(x)}{dx}$
            \item $\nabla_{\beta_N} \log q( \tau |\alpha_N , \beta_N) = \frac{\alpha_N}{\beta_N} - \tau$
        \end{itemize}
    \end{itemize}
\item $\log P(D, z[s]) = \log P(D, \mu_s , \tau_s) = \log P(D | \mu_s, \tau_s) + \log P(\mu_s , \tau_s)$
\end{itemize}

\subsubsection*{Question 1.2.5}
We implemented algorithm 1 of the BBVI paper using Pytorch\\
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\linewidth]{imgs/D3-AlgorithmBBVI.png}
\end{figure}
Generation of dataset
\begin{lstlisting}
def generate_data(mu, tau, N):
  x = torch.linspace(-10, 10, N)
  # Insert your code here
  sigma = 1 / torch.sqrt(torch.tensor(tau))   # precision τ = 1/σ²
  torch.manual_seed(10)
  
  D = torch.normal(mu, sigma, size=(N,))

  return D

mu = 1
tau = 0.5

dataset = generate_data(mu, tau, 100)
\end{lstlisting}
\newpage
Black Box Algorithm 1 implementation
\begin{lstlisting}
 class BlackBoxVI:
    """Black Box Variational Inference implementation."""
    
    def __init__(self, D, log_joint_distribution, variational_family_q,S=10, learning_rate=1e-3):
        """
        Black Box Variational Inference implementation without any .
            Args:
                D: dataset
                log_joint_distribution: function that computes the log joint distribution
                variational_family_q: variational family q with parameters to optimize
                S: number of samples for Monte Carlo estimation
                learning_rate: learning rate for the optimizer
        """
        self.D = D
        self.log_joint_distribution = log_joint_distribution
        self.variational_family_q = variational_family_q
        self.S = S
        self.learning_rate = learning_rate

    
    def fit(self, max_iterations = 1000):
        """Fit the variational parameters using BBVI algorithm.
        SGD optimization
        Args:
            threshold: convergence threshold
            max_iterations: maximum number of iterations
        """
        history = {
            "elbo" : [],
            "final_params" : None,
            'mu_expected': [],
            'tau_expected': [],
        }
        lr_sgd = self.learning_rate      # A small learning rate
        optimizer_sgd = optim.SGD([self.variational_family_q.parameters], lr=lr_sgd)
       
        for t in range(1,max_iterations+1):
            elbo=0
            loss=0
            optimizer_sgd.zero_grad()
            for s in range(self.S):
                z_s = self.variational_family_q.sample()

                #compute log(q(z[s]))
                log_q_z_s = self.variational_family_q.log_prob(z_s)
                
                # Compute log p(x, z[s])
                log_p_z_s_D = self.log_joint_distribution(self.D, z_s)
                
                # Compute the score function ∇_λ log q(z[s]; λ)
                learning_signal = (log_p_z_s_D - log_q_z_s).detach()
                
                #loss
                elbo += learning_signal
                loss += (- log_q_z_s * learning_signal)
            
            elbo /= self.S
            loss /= self.S
         
            loss.backward()
            optimizer_sgd.step()
            
            
            #we compute the elb and we check the params every 10 iterations and at the first iteration
            if t % 10 == 0 or t == 1:
                mu_N,lambda_N, alpha_N,beta_N = self.variational_family_q.get_actual_parameters()
                history["elbo"].append((t,elbo))
                history['mu_expected'].append((t,mu_N.item()))
                history['tau_expected'].append((t,(alpha_N / beta_N).item()))
            
        history['final_params'] = self.variational_family_q.get_parameters()
        
        return history
\end{lstlisting}
The optimization loop performs these steps at each iteration:
\begin{itemize}
    \item Sample latent variables $\rightarrow$ Draw $S$ samples $z[s]$ from the current variational distribution $q(z; \lambda)$
    \item Evaluate probabilities  $\rightarrow$ For each sample, we compute log $q(z[s]; \lambda)$ and log $p(D, z[s])$
    \item Compute learning signal $\rightarrow$ Calculate $f[s] = \log p(D, z[s]) - \log q(z[s])$, which represents how much better sample $z[s]$ explains the data compared to the variational approximation
    \item Calculate gradient $\rightarrow$  Use the score function estimator
    \item Update parameters $\rightarrow$  Use SGD to update $\lambda$ based on the averaged gradient
\end{itemize}


\begin{lstlisting}

class NormalGammaVariationalFamily():
    
    """Variational family for Normal-NormalGamma conjugate model.
    
        Variational distribution: q(μ, τ | λ) = q(μ | τ) q(τ)
        where:
            - μ | τ ~ Normal(μ_N, (λ_N * τ)^(-1))
            - τ ~ Gamma(α_N, β_N)
        
     Parameters: λ = [μ_N, λ_N, α_N, β_N]
    """
    def __init__(self):
        """Initialize with dimension of latent variable.""" 
        mu_N = torch.randn(1).item()
        lambda_N = torch.rand(1).item() * 2 + 0.5
        alpha_N = torch.rand(1).item() * 3 + 1.0
        beta_N = torch.rand(1).item() * 3 + 0.5
        
        self.parameters = torch.tensor([
            mu_N, torch.log(torch.tensor(lambda_N)), torch.log(torch.tensor(alpha_N)), torch.log(torch.tensor(beta_N))
        ], requires_grad=True)
        
    def get_actual_parameters(self):
        mu_N = self.parameters[0]
        
        lambda_N = torch.exp(self.parameters[1])
        alpha_N = torch.exp(self.parameters[2])
        beta_N = torch.exp(self.parameters[3])
        
        return mu_N, lambda_N, alpha_N, beta_N

    def get_parameters(self):
        return self.parameters.clone()
    
    def set_parameters(self,new_params):
        self.parameters = new_params.clone()
    

    def sample(self):
        """Sample from the variational distribution.
            z[s] ~ q(μ, τ | λ)
        """
       
        mu_N, lambda_N, alpha_N, beta_N = self.get_actual_parameters()

        tau = torch.distributions.Gamma(alpha_N, beta_N).sample()
        
        
        precision = lambda_N * tau
        
        sigma_mu = 1.0 / torch.sqrt(precision)
        
        mu = torch.distributions.Normal(mu_N, sigma_mu).sample()
        
        return (mu,tau)
    
    def log_prob(self, z):
        """Compute log probability of z under the variational distribution.
            Compute log q(μ, τ | λ).
         """
        mu, tau = z
        mu_N, lambda_N, alpha_N, beta_N = self.get_actual_parameters()

        
        log_q_tau = torch.distributions.Gamma(alpha_N, beta_N).log_prob(tau)
        
        
        precision = lambda_N * tau
        
        sigma_mu = 1.0 / torch.sqrt(precision)
        
        log_q_mu_given_tau = torch.distributions.Normal(mu_N,sigma_mu).log_prob(mu)

        return log_q_tau + log_q_mu_given_tau      
\end{lstlisting}
We store the logarithms of parametes to enforce the necessary constraint that these parameters must remain strictly positive during optimization, thereby allowing the optimization algorithm to operate effectively over the entire unconstrained range of real numbers.

\begin{lstlisting}
def log_joint_distribution(D, z):
        """Compute the log joint distribution log p(D, Z).
        Args:
            D: dataset
            Z: latent variables
        Returns:
            log p(D, Z)
        """
        # log p(D, Z) = log p(D|Z) + log p(Z)
        # Z = (mu, tau)
        
        mu, tau = z
        sigma = 1 / torch.sqrt(tau)
        
        mu_0 = 1.0
        lambda_0 = 0.1 
        a_0 = 1.0
        b_0 = 2.0  
        
        #log P(D|Z)
        log_likelihood = torch.distributions.Normal(mu, sigma).log_prob(D).sum()
        
        #log P(mu , tau) = log P(mu | tau) + log P(tau)
        
        # Log prior p(μ | τ)
        precision_mu = lambda_0 * tau
        sigma_mu = 1.0 / torch.sqrt(precision_mu)
        log_prior_mu = torch.distributions.Normal(mu_0, sigma_mu).log_prob(mu)
        
        # Log prior p(τ)
        log_prior_tau = torch.distributions.Gamma(a_0, b_0).log_prob(tau)
        
        
        return log_likelihood + log_prior_mu + log_prior_tau
\end{lstlisting}
The initial parameters have been taken from the last assignments ( $\mu_0 , \lambda_0 , \alpha_0, \beta_0)$\\
Application of the model
\begin{lstlisting}
q = NormalGammaVariationalFamily()

bbvi = BlackBoxVI(dataset,log_joint_distribution=log_joint_distribution,S = 30,variational_family_q=q,learning_rate=1e-5)
results = bbvi.fit(max_iterations=10**4)

print(results['mu_expected'][-1])
print(f"\n{'='*60}")
print(f"Final Results:")
print(f"E_q[μ] = {results['mu_expected'][-1][1]:.4f} (true: {mu})")
print(f"E_q[τ] = {results['tau_expected'][-1][1]:.4f} (true: {tau})")
print(f"Data mean: {dataset.mean():.4f}")
print(f"Data precision: {1.0/dataset.var():.4f}")
print(f"{'='*60}")

### Plot of the ELBO
plt.figure(figsize=(14, 6))
iterations, elbos = zip(*results["elbo"])
plt.plot(iterations,elbos,'b-')
plt.xlabel("Iteration")
plt.ylabel("Elbo")
plt.title('ELBO over Iterations', fontsize=14)
plt.grid(alpha=0.3)


### Plot for m
plt.figure(figsize=(14, 6))
iterations, mu_expected = zip(*results["mu_expected"])
plt.plot(iterations,mu_expected,'b-')
plt.axhline(mu, color='r', linestyle='--', label="real value")
plt.xlabel("Iteration")
plt.ylabel("mu")
plt.title('SGD: Convergence of Mean (m)', fontsize=14)
plt.grid(alpha=0.3)




### Plot for tau
plt.figure(figsize=(14, 6))
iterations, tau_expected = zip(*results["tau_expected"])
plt.plot(iterations,tau_expected,'b-')
plt.axhline(tau, color='r', linestyle='--', label="real value")
plt.xlabel("Iteration")
plt.ylabel("tau")
plt.title('SGD: Convergence of Tau', fontsize=14)
plt.grid(alpha=0.3)
\end{lstlisting}
\textbf{Conclusions}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/D3-ELBO.png}
    \caption{Evidence Lower Bound (ELBO) over training iterations.}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/D3-MeanConvergence.png}
    \caption{Convergence results for Tau parameter.}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/D3-TauConvergence.png}
    \caption{Convergence results for the Tau parameter.}
\end{figure}
\begin{table}[h!]
    \centering
    \label{tab:final_results}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Parameter} & \textbf{Variational Estimate ($E_q[\cdot]$)} & \textbf{True/Data Value} \\
        \hline
        Mean ($\mu$) & 0.8634 & 1.0000 \\
        \hline
        Precision ($\tau$) & 0.5255 & 0.5000 \\
        \hline
        Dataset Mean & N/A & 0.8901 \\
        \hline
        Dataset Precision & N/A & 0.4580 \\
        \hline
    \end{tabular}
    \caption{Data Statistics}
\end{table}

\section*{C Section}
\subsection*{C.1}
\subsubsection*{Question 2.1.10}
We want to show the the IWELBO is a valid lower bound on the log-marginal likelihood.
\[
\elbo_K = \E_{Z_1, ... ,Z_K }\left[\log \left( \frac{1}{K} \sum_{k=1}^{K} \frac{p(X,Z_k)}{q(Z_k | X)}\right) \right]
\]
\[
\elbo_K \leq \log p(X)
\]
We start the proof by applying the Jensen's equality
\[
    \E_Z \left[\log f(Z) \right] \leq \log \E_Z\left[f(Z)\right]
\]
The IWELBO becomes:
\begin{align*}
\E_{Z_1, ... ,Z_K }\left[\log \left( \frac{1}{K} \sum_{k=1}^{K} \frac{p(X,Z_k)}{q(Z_k | X)}\right) \right] &\leq \log \left( \frac{1}{K} \sum_{k=1}^{K} \E_{Z_K}\left[ \frac{p(X,Z_k)}{q(Z_k | X)} \right] \right)\\
& \leq \log  \left( \frac{1}{K} \sum_{k=1}^{K} \int \frac{P(Z_k, X)}{q(Z_k| X)} q(Z_k | X) dZ_k \right)\\
& \leq \log \frac{1}{K} \sum_{k=1}^{K} p(X)\\
& \leq \log P(X)
\end{align*}
This is the expected result
\[
\elbo_K \leq \log P(X)
\]

\subsubsection*{Question 2.1.11}
We want to prove that the IWELBO tightens the variational bound for K samples
\[
\elbo_K \geq \elbo_1  \quad \quad  K > 1
\]
We know that
\[
\elbo_K = \E_{Z_1, ... ,Z_K }\left[\log \left( \frac{1}{K} \sum_{k=1}^{K} \frac{p(X,Z_k)}{q(Z_k | X)}\right) \right]
\quad \quad \quad 
\elbo_1 = \E_Z\left[\log \frac{P(X,Z)}{q(Z)}\right]
\]
We start by observing
\[
\E_{Z_1, ... ,Z_K }\left[\log \left( \frac{1}{K} \sum_{k=1}^{K} \frac{p(X,Z_k)}{q(Z_k | X)}\right) \right] = \E_{Z_1, ... ,Z_K }\left[\log \E_{I={\{k_1\}}}\left[\frac{p(X,Z_{k_1})}{q(Z_{k_1} | X)}\right]\right]
\]
I is a set which has size equal to 1 ( it contains only one index $k_1$ which is randomly chosen from ${1 , 2 , . . . , K}$ with equal probability $\frac{1}{K}$)
\[
\E_{I={\{k_1\}}}\left[\frac{a_{k_1}}{1}\right] = \frac{a_1 + ... + a_k}{k}
\]
In other words, it is another way of expressing the sum.\\
Then,we apply the Jensen's inequality
\[
\elbo_K \geq  \E_{Z_1, ... ,Z_K }\left[\E_{I={\{k_1\}}}\left[\log \frac{p(X,Z_{k_1})}{q(Z_{k_1} | X)}\right]\right]
\]
We invert the two expectations
\[
\elbo_K \geq  \E_{I={\{k_1\}}}\left[\E_{Z_{k_1}}\left[\log \frac{p(X,Z_{k_1})}{q(Z_{k_1} | X)}\right]\right]
\]
Since I uniformly select one index from ${1 , . . . , K}$ and each $z_k$ is i.i.d from $q(z|x)$ and the inner expectation gives the same value for any choice of ${k_1}$
\begin{align*}
\E_{I={\{k_1\}}}\left[\E_{Z_{k_1}}\left[\log \frac{p(X,Z_{k_1})}{q(Z_{k_1} | X)}\right]\right] = \E_z\left[\log \frac{P(X,Z)}{q(Z|X)}\right] = \elbo_1
\end{align*}
We have proved that
\[
\elbo_k \geq \elbo_1
\]

\subsection*{C.2}
\subsubsection*{Question 2.1.12}
Considering the model described below and the mean-field approximation, we derive the Rao-Blackwellized partial gradient of the ELBO w.r.t $\lambda_3$ 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\linewidth]{imgs/C2-RaoBlackwellizedModel.png}
    \caption{PGM of some generic model for Rao-Blackwellization}
\end{figure}
\[
q(w_1, w_2, z, v, y) = q_{\lambda_1}(w_1)q_{\lambda_2}(w_2)q_{\lambda_3}(z)q_{\lambda_4}(v) \prod_{n} q_{\lambda_{5,n}}(y_n).
\]
\[
    \nabla_{\lambda_3} \elbo = E_{q_{(3)}} \left[ \nabla_{\lambda_3} \log q(z_3 | \lambda_3) \left( \log p_3(x,z_{(3)}) - \log q(z_3| \lambda_3) \right) \right]
\]
Where:
\begin{itemize}
    \item $z_3 = z$
    \item $z_{(3)} = {z_3} \cup \text{Markov Blanket of } z_3=  \{z, v , x_n , y_n \} \forall n$
    \begin{itemize}
        \item Markov Blanket of a variable $z_3$ is all the parents, co-parents and children of $z_3$ $\rightarrow \{v , x_n , y_n \} \forall n$ 
    \end{itemize}
    \item $\log p_{3}(x, z)$ is the the portion of the joint probability that depends on $z_3 = z$ 
\[
\log p_3(x, z) = \log p(z | v) + \sum_{n=1}^{D} \log p(x_n | z, y_n) 
\]
    \item $q_{(3)}$ is the distribution of variables in the model that depend on the third variable z  ( the Markov Blanket of z and z )
\[
q_{(3)}(z_{(3)}) = q_{(3)}(z) = \prod_{z_j \in z_{(3)}} q(z_j | \lambda_j) =  q(z | \lambda_3) \cdot q(v | \lambda_4) \cdot \prod_{n} q(y_n | \lambda_{5,n})
\]

\subsection*{C.3}
\subsubsection*{Question 2.2.13}
We extend the implementation of problem 1.D.3 with the Control Variate used in the BBVI paper ( without Rao-blackwellization ).
We provide also the same plots seen for 1.D.3.
Preliminare informations:
\begin{itemize}
    \item D is the number of variational parameters $\lambda =\{ \mu_N , \lambda_N , \alpha_N , \beta_N\}$
    \item Z is $\{\mu , \tau\}$
    \item S is the number of Monte Carlo samples
\end{itemize}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.32\linewidth]{imgs/C3-BBVIAlgorithm2.jpg}
    \caption{BBVI II}
\end{figure}
We just ignore the pedix of p and q in the algorithm.
The generation code is the same of the 1.D.3
\begin{lstlisting}
class BlackBoxVI:
    """Black Box Variational Inference implementation."""
    
    def __init__(self, D, log_joint_distribution, variational_family_q,S=10, learning_rate=1e-3):
        """
        Black Box Variational Inference implementation without any .
            Args:
                D: dataset
                log_joint_distribution: function that computes the log joint distribution
                variational_family_q: variational family q with parameters to optimize
                S: number of samples for Monte Carlo estimation
                learning_rate: learning rate for the optimizer
        """
        self.D = D
        self.log_joint_distribution = log_joint_distribution
        self.variational_family_q = variational_family_q
        self.S = S
        self.learning_rate = learning_rate

    
    def fit(self, max_iterations = 1000):
        """Fit the variational parameters using BBVI algorithm.
        SGD optimization
        Args:
            threshold: convergence threshold
            max_iterations: maximum number of iterations
        """
        history = {
            "elbo" : [],
            "final_params" : None,
            'mu_expected': [],
            'tau_expected': [],
        }
        lr_sgd = self.learning_rate      # A small learning rate
        optimizer_sgd = optim.SGD([self.variational_family_q.parameters], lr=lr_sgd)
       
        for t in range(1,max_iterations+1):
            elbo=0
            loss=0
            optimizer_sgd.zero_grad()
            
            number_of_variational_parameters = self.variational_family_q.get_number_of_parameters()

            f = torch.zeros((number_of_variational_parameters, self.S))
            h = torch.zeros((number_of_variational_parameters, self.S))

            for s in range(self.S):
                z_s = self.variational_family_q.sample()

                #compute log(q(z[s]))
                log_q_z_s = self.variational_family_q.log_prob(z_s)
                
                # Compute log p(x, z[s])
                log_p_z_s_D = self.log_joint_distribution(self.D, z_s)
                
                # Compute the score function ∇_λ log q(z[s]; λ)
                learning_signal = (log_p_z_s_D - log_q_z_s).detach()
                
                #loss
                elbo += learning_signal

                # We need to compute ∇_λ log q(z[s]; λ) for each parameter
                grad_log_q_z_s = torch.autograd.grad(
                        log_q_z_s, 
                        self.variational_family_q.parameters, 
                        retain_graph=True, 
                        create_graph=False)[0]

                 # For each variational parameter d
                for d in range(number_of_variational_parameters):
                    # Control variate computation
                    grad_d = grad_log_q_z_s[d]
                    
                    # Compute f_t and h_t for this sample
                    f[d, s] += learning_signal * grad_d
                    h[d, s] += grad_d 
                
            elbo /= self.S
                
            
            # Compute gradient with control variates for each parameter d
            final_gradient = torch.zeros(number_of_variational_parameters)
            
            for d in range(number_of_variational_parameters):
                # a_d* = Cov(f_d, h_d) / Var(h_d)
                
                f_d_mean = f[d].mean()
                h_d_mean = h[d].mean()
                
                # Covariance
                cov_f_h = ((f[d] - f_d_mean) * (h[d] - h_d_mean)).mean()
                
                # Variance of h_d
                var_h = ((h[d] - h_d_mean) ** 2).mean()
                
                # We add small epsilon to avoid division by zero
                if var_h > 1e-8:
                    a_d_star = cov_f_h / var_h
                else:
                    a_d_star = 0.0
                
                # ∇_λd L ≈ (1/S) Σ[f_i[s] - a_d* h_i[s]]
                final_gradient[d] = (f[d] - a_d_star * h[d]).mean()
            
            with torch.no_grad():
                self.variational_family_q.parameters += self.learning_rate * final_gradient
            
            #we compute the elb and we check the params every 10 iterations and at the first iteration
            if t % 10 == 0 or t == 1:
                mu_N,lambda_N, alpha_N,beta_N = self.variational_family_q.get_actual_parameters()
                history["elbo"].append((t,elbo))
                history['mu_expected'].append((t,mu_N.item()))
                history['tau_expected'].append((t,(alpha_N / beta_N).item()))
            
        history['final_params'] = self.variational_family_q.get_parameters()
        
        return history
            
\end{lstlisting}
The rest of the code follows the same structure as before and is omitted for brevity ( the entire code can be found in the appendix )

\textbf{Brief explanation of the changes}\\
For each Monte Carlo sample $s$, the algorithm computes two critical matrices:

\textbf{$f[d, s]$} represents the ``learning signal times gradient'' for parameter $d$ at sample $s$. Specifically:
\begin{itemize}
    \item First, compute the learning signal: $\log p(D, z[s]) - \log q(z[s]; \lambda)$, which measures how well sample $z[s]$ explains the data
    \item Then compute $\nabla_{\lambda} \log q(z[s]; \lambda)$ using PyTorch's autograd, giving gradients for all parameters
    \item For each parameter $d$: $f[d, s] = \text{learning\_signal} \times \text{grad}_d$
\end{itemize}

\textbf{$h[d, s]$} stores just the score function gradient for parameter $d$ at sample $s$:
\begin{equation}
h[d, s] = \nabla_{\lambda_d} \log q(z[s]; \lambda)
\end{equation}

These matrices have dimensions $[\text{number\_of\_parameters} \times S]$, storing all gradient information across samples.

The control variates method exploits a key mathematical property: $\mathbb{E}[h[d,s]] = 0$ under the variational distribution $q$. This means $h$ can serve as a \textbf{zero-mean baseline} that, when properly scaled, reduces variance without introducing bias.

\textbf{Conclusions}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/C3-ELBO.png}
    \caption{Evidence Lower Bound (ELBO) over training iterations.}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/C3-MeanConvergence.png}
    \caption{Convergence results for Tau parameter.}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/C3-TauConvergence.png}
    \caption{Convergence results for the Tau parameter.}
\end{figure}
\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Parameter} & \textbf{Variational Estimate ($E_q[\cdot]$)} & \textbf{True/Data Value} \\
        \hline
        Mean ($\mu$) & 0.8939 & 1.0000 \\
        \hline
        Precision ($\tau$) & 0.5269 & 0.5000 \\
        \hline
        Dataset Mean & N/A & 0.8901 \\
        \hline
        Dataset Precision & N/A & 0.4580 \\
        \hline
    \end{tabular}
    \caption{Data Statistics}
\end{table}
\end{itemize}
\newpage
The variance reduction results in more stable parameter updates, allowing the algorithm to take more confident steps toward the optimum. This is particularly evident in the steeper initial descent and the reduced noise in later iterations, confirming that control variates substantially improve the efficiency and reliability of the BBVI optimization process.
\newpage
\subsection*{C.4}
We study the Gamma distribution and use it exponential-family structure to derive the Fish Information Matrix ( FIM ) and implement Natural Gradient Descent (NGD).\\
We consider the Gamma distribution parameterized by shape $\alpha > 0$ and $\beta > 0$
\[
p(x \mid \alpha, \beta) = \frac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha-1} \exp\left(-\frac{x}{\beta}\right), \quad x > 0.
\]
\subsubsection*{Question 2.2.14}
We rewrite the Gamma Distribution in canonical exponential-family form identifying the natural parameters $\eta$ , sufficient statistics $t(x)$ and log-normalizer $A(\eta)$.\\
We also derive gradient $\nabla_\eta A(\eta)$
\[
p(x | \eta) = h(x) e^{\eta^T t(x) - A(\eta)}
\]
We start by consideration the definition of Gamma Distribution pdf 
\begin{align*}
p(x| \alpha, \beta) &= \frac{1}{\Gamma(\alpha)} x^{\alpha-1} e^{\frac{-x}{\beta} -\alpha \log \beta}\\
&= x^{\alpha-1} e^{\frac{-x}{\beta} -\alpha \log \beta - \log \Gamma(\alpha)}\\
&= e^{-\frac{x}{\beta} +(\alpha-1) \log x  -(\alpha \log \beta + \log \Gamma(\alpha))}
\end{align*}
Where:
\begin{itemize}
    \item $t(x) = \binom{x}{\log x}$
    \item $\eta = \binom{-\frac{1}{\beta}}{\alpha -1} = \binom{\eta_1}{\eta_2}$
    \item $\beta = -\frac{1}{\eta_1}$
    \item $\alpha=\eta_2+1$
    \item $A(\eta)$
\begin{align*}
A(\eta) &= \alpha \log \beta + \log \Gamma(\alpha)\\
&= -(\eta_2 + 1) \log (-\eta_1) + \log \Gamma(\eta_2+1)
\end{align*}
    \item $\nabla_\eta A(\eta)$
\[
\nabla_\eta A(\eta) = \begin{bmatrix}
\nabla_{\eta_1} A(\eta) \\
\nabla_{\eta_2} A(\eta)
\end{bmatrix}
= \begin{bmatrix}
-\frac{(\eta_2 + 1)}{\eta_1} \\
-\log(-\eta_1) + \frac{\Gamma'(\eta_2+1)}{\Gamma(\eta_2+1)}
\end{bmatrix}
= \begin{bmatrix}
+\beta \lambda \\
+\log(\beta ) + \Psi(a)
\end{bmatrix} = \begin{bmatrix}
E[x] \\
E[\log x]
\end{bmatrix}
\]
Note: Digamma Function
\[
\Psi(z) = \frac{d\Gamma(z)}{\Gamma(z)}
\]

\end{itemize}

\subsubsection*{Question 2.2.15}
We are considering MSE as the differentiable loss function $L(\alpha,\beta)$
\[
L(\alpha,\beta) = \frac{1}{N} \sum_{i=1}^{N} (x_i - \tilde{x_i}(\alpha,\beta))^2
\]
We use the properties of exponential-family distributions and the Fisher Information $G(\alpha,\beta)$ for expressing the natural gradient $\tilde{\nabla}_{(\alpha,\beta)} \elbo$ as a function of the standard gradient $\nabla_{(\alpha,\beta)} \elbo$ and the FIM\\
The general formula is given from the Stochastic Variatonal Inference paper of Hoffman, Blei, Wang and Paisley:
\[
\tilde{\nabla}_{(\alpha,\beta)} \elbo = G(\alpha,\beta)^{-1} \nabla_{(\alpha,\beta)} \elbo
\]
In particular, for exponential-family distribution, we can espress the relation between the two types of gradients as:\\
\[
\tilde{\nabla}_{(\alpha,\beta)} \elbo = \left[\nabla_{(\alpha,\beta)}^2 A(\alpha,\beta) \right]^{-1} \nabla_{(\alpha,\beta)} \elbo
\]

\subsubsection*{Question 2.2.16}
We compute the inverse Fisher Information Matrix $F(\alpha,\beta)^{-1}$ explicitly. We verify your expression numerically.

\begin{align*}
G(\alpha,\beta) &= \nabla_{\alpha,\beta}^2 A(\alpha,\beta) \\
G(\eta) &= \nabla_{\eta}^2 A(\eta)\\
&= \begin{pmatrix}
\nabla_{\eta_1} \nabla_{\eta_1} A(\eta) & \nabla_{\eta_1} \nabla_{\eta_2} A(\eta) \\
\nabla_{\eta_1} \nabla_{\eta_2} A(\eta) & \nabla_{\eta_2} \nabla_{\eta_2} A(\eta)
\end{pmatrix}\\
&= \begin{pmatrix}
\frac{(\eta_2+1)}{\eta_1^2} & -\frac{1}{\eta_1} \\
-\frac{- 1}{(-\eta_1)} & \psi'(\eta_2 + 1)
\end{pmatrix} \\
&= \begin{pmatrix}
\frac{\eta_2 + 1}{\eta_1^2} & -\frac{1}{\eta_1} \\
-\frac{1}{\eta_1} & \psi'(\eta_2 + 1)
\end{pmatrix}
\end{align*}

Knowing that $\eta_1 = -\frac{1}{\beta}$ and $\eta_2 = \alpha -1$, we can express $G(\eta)$ as $G(\alpha,\beta)$:
\begin{align*}
G(\alpha,\beta) &= \begin{bmatrix}
\beta^2 \alpha & \beta \\
\beta & \psi'(\alpha)
\end{bmatrix}
\end{align*}
Then, we use the inverse fomula for 2x2 matrices
\begin{align*}
E &= \begin{pmatrix} a & b \\ c & d \end{pmatrix} = \begin{bmatrix}
\beta^2 \alpha & \beta \\
\beta & \psi'(\alpha)
\end{bmatrix}
\end{align*}

\begin{align*}
E^{-1} &= \frac{1}{ad - bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\end{align*}

\begin{align*}
&= \frac{1}{\beta^2(\alpha \psi'(\alpha) - 1)} \begin{bmatrix}
\psi'(\alpha) & -\beta \\
-\beta & \beta^2 \alpha
\end{bmatrix} \\
&= \frac{1}{\alpha \psi'(\alpha) - 1} \begin{bmatrix}
\frac{\psi'(\alpha)}{\beta^2} & -\frac{1}{\beta} \\
-\frac{1}{\beta} & \alpha
\end{bmatrix}
\end{align*}

We choose $\alpha = 2$ and $\beta = 3$

$\psi'(\alpha) = 0.6449$

\begin{align*}
G(2,3) &= \begin{pmatrix}
18 & 3 \\
3 & 0.64473
\end{pmatrix}
\end{align*}
\begin{align*}
G^{-1}(2,3) &= \frac{1}{0.2878} \begin{pmatrix}
0.071653 & -\frac{1}{3} \\
-\frac{1}{3} & 2
\end{pmatrix}
= \begin{pmatrix}
0.24922 & -1.15022 \\
-1.15022 & 6.90734
\end{pmatrix}
\end{align*}
We verify numerically that : $GG^{-1} = I$
\begin{align*}
\begin{pmatrix}
18 & 3 \\
3 & 0.64473
\end{pmatrix}
\begin{pmatrix}
0.24922 & -1.15022 \\
-1.15022 & 6.90734
\end{pmatrix}
= \begin{pmatrix}
1 & \approx 0 \\
\approx 0 & 1
\end{pmatrix}
\end{align*} 
The Fisher information for the Gamma distribution can take slightly different forms depending on whether the parameter $\beta$, which chould be interpreted as
\begin{itemize}
    \item a scale parameter (as in the assignment PDF) $\text{Gamma}(\alpha, \beta_\text{scale})$
    \item as a rate parameter (as used in the instructor’s code) $\text{Gamma}(\alpha, \lambda_\text{rate})$ where $\lambda_\text{rate} = \frac{1}{\beta_\text{scale}}$
\end{itemize}
Since the exponential-family formulation depends on this choice, the corresponding Fisher matrices appear different at first glance.\\
In this report, I followed the definition of $\beta$ provided in the PDF, where $\beta$ represents the scale. When converting to the parameterization used in the instructor’s implementation (where 
$\beta$ is treated as a rate-like quantity ) the expressions align perfectly after applying the appropriate change of variables.
Thus, the discrepancy is purely due to the difference in parameter meaning (scale vs. rate), and not to an error in the derivation.

\subsubsection*{Question 2.2.17}
Assuming $x_{1:N}$ as i.i.d. samples from $\text{Gamma}(\alpha*, \beta*)$, we want to write down the gradient of the following average negative log-likelihood wrt $(\alpha,\beta)$ 
\begin{align*}
\elbo(\alpha,\beta) &= -\frac{1}{N} \sum_{n=1}^{N} \log p(x_n|\alpha, \beta)\\
&= -\frac{1}{N} \sum_{n=1}^{N} \log \left(\frac{1}{\Gamma(\alpha) \beta^\alpha} x_n^{\alpha-1} e^{-\frac{x_n}{\beta}} \right)\\
&= - \frac{1}{N} \sum_{n=1}^{N} \left[ - \alpha \log \beta - \log \Gamma(\alpha) +(\alpha-1) \log x_n - \frac{x_n}{\beta}  \right]\\
&= +\alpha \log \beta + \log \Gamma(\alpha) - \frac{1}{N} \sum_{n=1}^{N} \left[ +(\alpha-1) \log x_n - \frac{x_n}{\beta}  \right]\\
&= +\alpha \log \beta + \log \Gamma(\alpha) + (\alpha -1) \overline{\log x} + \frac{\overline{x}}{\beta}
\end{align*}
Where:
\begin{itemize}
    \item $\overline{\log x}$ is the empirical mean wrt to $\log x \rightarrow \frac{1}{N}\sum_{n=1}^{N} \log x_n$
    \item $\overline{x}$ is the empirical mean wrt to $x \rightarrow \frac{1}{N} \sum_{n=1}^{N} x_n$
\end{itemize}
Now, we compute its gradient
\begin{align*}
    \nabla_{(\alpha,\beta)} \elbo (\alpha,\beta) &= \binom{\nabla_\alpha \elbo(\alpha, \beta)}{\nabla_\beta \elbo(\alpha, \beta)}\\
    &= \binom{\log \beta + \psi(\alpha) - \overline{\log x}}{\frac{\alpha}{\beta} - \frac{\overline{x}}{\beta^2}}
\end{align*}
$\psi = \left(\log \Gamma(\alpha)\right)' =\frac{d \Gamma(\alpha)}{\Gamma(\alpha)}$

\subsubsection*{Question 2.2.18}
We implement a python notebook for estimating $(\alpha,\beta)$ for a $\text{Gamma}(\alpha^*=3.0, \beta^*=2.0)$ with a dataset of 1000 points and an initial poor guess. We use both standard gradient descent (GD) and natural gradient descent (NGD) ensuring $\alpha > 0$ and  $\beta > 0$ during optimization\\
Definition of hyperparameters and some model settings
\begin{lstlisting}
import torch
import numpy as np
import matplotlib.pyplot as plt
from torch.distributions import Gamma
# ------------------------

# True parameters of the Gamma distribution we want to discover
ALPHA_TRUE = 3.0   # shape
BETA_TRUE  = 2.0   # scale

# Number of observed data points
N_DATA = 1000

# Optimization parameters
LEARNING_RATE = 0.1
EPOCHS = 150

# Initial "wrong" guess for our parameters (still positive)
ALPHA_INIT = 0.5
BETA_INIT  = 8.0

# Fix random seed for reproducibility (optional)
torch.manual_seed(0)
\end{lstlisting}

\begin{lstlisting}
# Our parameterisation is shape-scale, but PyTorch's Gamma uses shape-rate.
rate_true = 1.0 / BETA_TRUE
dist_true = Gamma(concentration=torch.tensor(ALPHA_TRUE),
                  rate=torch.tensor(rate_true))

data = dist_true.sample((N_DATA,))

print(f"Generated {N_DATA} data points from Gamma(alpha={ALPHA_TRUE}, beta={BETA_TRUE})")
print(f"Sample mean:     {data.mean().item():.4f}")
print(f"Sample variance: {data.var().item():.4f}\n")
\end{lstlisting}
Implementation of the negative log-likelihood lossfunction for the Gamma disrribution.
\begin{lstlisting}
# ToDo: Loss function
def gamma_nll(alpha, beta, data_points):
    """
    ToDo:
        Implement the average negative log-likelihood for Gamma distribution with shape=alpha and scale=beta.

        Hints:
        - Enforce positivity using clamp (e.g. min=1e-4).
        - PyTorch's Gamma takes (concentration=alpha, rate=1/beta).
        - Return the *mean* negative log-likelihood.

    """
    alpha = torch.clamp(alpha, min=1e-4)
    beta = torch.clamp(beta, min=1e-4)
    
    # Convert data_points to tensor if not already
    data_tensor = torch.tensor(data_points) if not torch.is_tensor(data_points) else data_points
    
    empirical_mean_x = data_tensor.mean()
    empirical_mean_log_x = torch.log(data_tensor).mean()
    nll = +alpha*torch.log(beta) + torch.lgamma(alpha) - (alpha-1)*empirical_mean_log_x + (1/beta)*empirical_mean_x
    return nll
\end{lstlisting}

\begin{lstlisting}
# Parameters for Standard Gradient Descent (GD)
alpha_gd = torch.tensor(ALPHA_INIT, requires_grad=True)
beta_gd  = torch.tensor(BETA_INIT,  requires_grad=True)

# Parameters for Natural Gradient Descent (NGD)
alpha_ngd = torch.tensor(ALPHA_INIT, requires_grad=True)
beta_ngd  = torch.tensor(BETA_INIT,  requires_grad=True)

# History trackers
history_gd  = []
history_ngd = []
history_loss_gd =[]
history_loss_ngd = []
\end{lstlisting}
We detach because the Fisher Information matrix (and its inverse) is a curvature estimate used to precondition the gradient updates. It should not be part of the computational graph for gradient computation.
\begin{lstlisting}
    def fisher_inverse(alpha, beta):
    """
    TODO:
      Implement the inverse Fisher Information matrix F^{-1}(α, β)
      for the Gamma(shape=α, scale=β) distribution.

      Theory:
        F(α, β) =
            [ ψ1(α)        1/β      ]
            [  1/β     α/β^2        ]

        F^{-1}(α, β) =
            1 / (α ψ1(α) - 1) *
            [  α           -β             ]
            [  -β       β^2 ψ1(α)         ]

      Hints:
      - Use torch.polygamma(1, alpha) for ψ1(α) (trigamma).
      - Make sure to detach alpha, beta so F^{-1} is not part of the graph.
    """
    alpha_detached = alpha.detach()
    beta_detached = beta.detach()
    
    external_factor = 1/(alpha_detached* torch.polygamma(1, alpha_detached)-1)
    inv11 = alpha_detached * external_factor
    inv12 = -beta_detached * external_factor
    inv22 = beta_detached**2 *  torch.polygamma(1, alpha_detached) * external_factor
    return inv11, inv12, inv22
\end{lstlisting}
We run the optimization loop for a specified number of epochs. In each epoch, we perform both standard gradient descent and natural gradient descent updates. We compute the gradients, build the Fisher Information Matrix for NGD, and update the parameters accordingly. We also log the parameter values and losses at regular intervals.
\begin{lstlisting}
    print(f"Optimizing with LR={LEARNING_RATE} for {EPOCHS} epochs...")

for epoch in range(EPOCHS):

    # ===== A. Standard Gradient Descent (GD) =====

    if alpha_gd.grad is not None:
        alpha_gd.grad.zero_()
    if beta_gd.grad is not None:
        beta_gd.grad.zero_()

    loss_gd = gamma_nll(alpha_gd, beta_gd, data)
    
    loss_gd.backward()

    with torch.no_grad():
        alpha_gd -= LEARNING_RATE * alpha_gd.grad
        beta_gd  -= LEARNING_RATE * beta_gd.grad

        alpha_gd.clamp_(min=1e-4)
        beta_gd.clamp_(min=1e-4)
        
    history_loss_gd.append(loss_gd.item())
    history_gd.append((alpha_gd.item(), beta_gd.item()))

    # ===== B. Natural Gradient Descent (NGD) =====

    if alpha_ngd.grad is not None:
        alpha_ngd.grad.zero_()
    if beta_ngd.grad is not None:
        beta_ngd.grad.zero_()

    loss_ngd = gamma_nll(alpha_ngd, beta_ngd, data)
    loss_ngd.backward()

    g_alpha = alpha_ngd.grad
    g_beta  = beta_ngd.grad

    # ToDo : compute natural gradient using F^{-1}(α, β)
    #  1) Get F^{-1} entries using fisher_inverse(...)
    #  2) Compute:
    #       - ng_alpha
    #       - ng_beta

    inv11, inv12, inv22 = fisher_inverse(alpha_ngd, beta_ngd)

    ng_alpha =  inv11*g_alpha + inv12*g_beta
    ng_beta  =  inv12*g_alpha + inv22*g_beta

    with torch.no_grad():
        alpha_ngd -= LEARNING_RATE * ng_alpha
        beta_ngd  -= LEARNING_RATE * ng_beta

        alpha_ngd.clamp_(min=1e-4)
        beta_ngd.clamp_(min=1e-4)
    
    history_loss_ngd.append(loss_ngd.item())
    history_ngd.append((alpha_ngd.item(), beta_ngd.item()))

    if (epoch + 1) % 15 == 0 or epoch == 0:
        print(f"\n--- Epoch {epoch + 1} ---")
        print(f"  GD:  alpha={alpha_gd.item():.4f}, beta={beta_gd.item():.4f}, "
              f"Loss={loss_gd.item():.4f}")
        print(f"  NGD: alpha={alpha_ngd.item():.4f}, beta={beta_ngd.item():.4f}, "
              f"Loss={loss_ngd.item():.4f}")


print("\nOptimization finished.")
\end{lstlisting}
To illustrate the difference between standard gradients and natural gradients, we print out the gradients computed in the first epoch for both methods.
\begin{lstlisting}
hist_gd_np  = np.array(history_gd)
hist_ngd_np = np.array(history_ngd)
hist_loss_gd = np.array(history_loss_gd)
hist_loss_ngd = np.array(history_loss_ngd)


fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True)
fig.suptitle(f"Gamma: Standard Gradient vs. Natural Gradient "
             f"(LR={LEARNING_RATE}, N={N_DATA})", fontsize=16)

# Plot 1: alpha (shape)
ax1.plot(hist_gd_np[:, 0],  label="GD alpha",  color='blue', linestyle='--')
ax1.plot(hist_ngd_np[:, 0], label="NGD alpha", color='red')
ax1.axhline(ALPHA_TRUE, color='black', linestyle=':', label=f"True alpha ({ALPHA_TRUE})")
ax1.set_ylabel("Shape parameter $\\alpha$")
ax1.legend()
ax1.grid(True)

# Plot 2: beta (scale)
ax2.plot(hist_gd_np[:, 1],  label="GD beta",  color='blue', linestyle='--')
ax2.plot(hist_ngd_np[:, 1], label="NGD beta", color='red')
ax2.axhline(BETA_TRUE, color='black', linestyle=':', label=f"True beta ({BETA_TRUE})")
ax2.set_xlabel("Epoch")
ax2.set_ylabel("Scale parameter $\\beta$")
ax2.legend()
ax2.grid(True)

# Plot 3: Negative Log-Likelihood (NLL) Loss
ax3.plot(hist_loss_gd,  label="GD NLL Loss",  color='blue', linestyle='--')
ax3.plot(hist_loss_ngd, label="NGD NLL Loss", color='red')
ax3.set_xlabel("Epoch")
ax3.set_ylabel("NLL Loss")
ax3.legend()
ax3.grid(True)



plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
\end{lstlisting}
\newpage
\subsubsection*{Question 2.2.19}
We plot and compare the convergence trajectories of GD and NGD for both parameters $(\alpha_t , \beta_t)$, as well as the evolution of the negative log-likelihood
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/C4-NGDvsGD.png}
\end{figure}
\\
\textbf{Introduction}:
The natural gradient is the fastest decreasing direction of the error defined in the parameter space in the Riemannian space\\
The Fisher information matrix (positive definite) defines the local curvature of the model distribution space\\
\textbf{Answer}: The natural gradient leads to faster and more stable convergence because it corrects the direction of the standard gradient by taking into account the curvature of the probability distribution's manifold (via the Fisher Information Matrix, G).
By multiplying the standard gradient with the inverse of the FIM , NGD scales down updates along shallow directions and amplifies updates along directions that cause a large change in the distribution (even if the standard gradient is small), effectively making the optimization landscape appear more like a sphere


\section*{B Section}
\subsection*{B.1}
\subsubsection*{Question 3.1.20}
We know that the categorical distribution is discrete and non-differentiable, making it impossible to backpropagate gradients through samples during training.
We can use the Gumber-Max trick which provides a simple and efficient way to draw samples z from a categorical distribution
\[
z = \text{one hot}(\arg \max_i\left[ g_i + \log \pi_i \right])
\]
Where 
\begin{itemize}
    \item $\pi_1 . . . \pi_K$ are the class probabilities ( K is the number of class probabilities )
    \item $g_1 . . . g_K$ are i.i.d. samples drawn from $\text{Gumbel}(0,1)$
\end{itemize}
But it is not differentiable: we can approximate the arg max function with a differentiable continuous function called softmax in order to generate k-dimensional sample vectors k-dimensional sample vectors $y \in \Delta^{k-1}$ ( one hot encoding )
\[
y_i = \frac{e^{\frac{\log \pi_i + g_i}{\tau}}}{\sum_{j=1}^{k}e^{\frac{\log \pi_j + g_j}{\tau}}}
\]
We can define the density of the Gumbel-Softmax distribution as
\[
p_{\pi, \tau}(y_1 , . . . , y_k) = \Gamma(k) \tau^{k-1} \left( \sum_{i=1}^{K} \frac{\pi_i}{y_i^\tau} \right)^{-k} \prod_{i=1}^{K} \frac{\pi_i}{y_i^{\tau+1}}
\]
As the softmax temperature $\tau$ approaches 0 $\tau \rightarrow 0$, samples from the Gumbel Softmax distribution become one-hot and the Gumbel-Softmax distribution becomes identical to the categorical distribution $p(z)$. As the temperature increase $\tau \rightarrow \infty$ , the samples becomes uniform .
Below write a function that generates N samples from Categorical $a$, where $a = [a_0, a_1, a_2, a_3]$.
\begin{lstlisting}
def categorical_sampler(a, N):
  samples = torch.distributions.Categorical(a).sample((N,))

  return samples  # should be N-by-1
\end{lstlisting}

The option dim=-1 normalizes across the K categories for each sample ( each row (representing one sample) will sum to 1)
\begin{lstlisting}
# Hint: approximate the Categorical distribution with the Gumbel-Softmax distribution
def categorical_reparametrize(a, N, temp=0.1, eps=1e-20):  # temp and eps are hyperparameters for Gumbel-Softmax
  g_gumbel_0_1 = torch.distributions.Gumbel(0,1).sample((N,len(a)))
  a.requires_grad = True
  x = torch.log(a + eps) + g_gumbel_0_1
  
  samples = torch.nn.functional.softmax(x / temp, dim=-1)

  return samples # make sure that your implementation allows the gradient to backpropagate
\end{lstlisting}
Comparing the samples
\begin{lstlisting}
    a = torch.tensor([0.1,0.2,0.5,0.2])
    N = 1000
    direct_samples = categorical_sampler(a, N)
    reparametrized_samples = categorical_reparametrize(a, N, temp=0.1, eps=1e-20)

    hard_samples = reparametrized_samples.argmax(dim=1,keepdim=True)
    compare_samples(direct_samples, hard_samples, bins=4)
\end{lstlisting}
\textbf{Results}\\
The graph shows that both methods produce similar distributions (mostly orange visible means they overlap well), which validates that our Gumbel-Softmax implementation is working correctly!
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/B1-Comparison.png}
    \caption{Comparison of samples from direct categorical sampling vs. Gumbel-Softmax reparameterization}
\end{figure}
\newpage

\newpage
\subsection*{B.2}
\subsubsection*{Question 3.1.21}
Following the methodology from the Sticking the Landing paper, I derived the gradient of the ELBO under the reparameterization trick and showed how it can be decomposed into multiple terms, one of which is the score function. This decomposition reveals the connection between the reparameterization gradient and the score function gradient.


\end{document}
