\documentclass[a4paper]{scrartcl}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\elbo}{\mathcal{L}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\q}{q}

\title{Assignment 2E, 2025}
\subtitle{DD2434 Machine Learning, Advanced Course}
\author{
Riccardo Alfonso Cerrone \\
  \texttt{cerrone@kth.se}
\and
Bruno Carchia \\
  \texttt{carchia@kth.se}
}

\begin{document}
\maketitle
\section*{2.1 \quad Exponential Family}
\subsection*{Question 2.1.1}
The Poisson distribution is
\[
p(x\mid \lambda)
 = \frac{\lambda^x}{x!} e^{-\lambda}.
\]
We can rewrite it as:
\[
p(x\mid\lambda)
 = \frac{1}{x!}e^{\!\left(x\log\lambda - \lambda\right)},
 = h(x) e^{\eta^T t(x) - a(\eta)}
 \]
We assume
\[ \eta = \log \lambda\]
\[ \lambda = e^{\eta}\]
\[
h(x)=\frac{1}{x!},\qquad
t(x)=x,\qquad
a(\eta)=e^{\eta}.
\]
Final result 
\[
p(x|\lambda) = \frac{1}{x!} e^{x\eta - e^{\eta}}
\]

\subsection*{Question 2.1.2}
By definition , we know that:
\begin{align*}
  G(\lambda)
  &=  \mathbb{E}_{\lambda}\!\left[
        (\nabla_{\lambda} \log p(\beta \mid \lambda))
        (\nabla_{\lambda} \log p(\beta \mid \lambda))^{\top}
      \right]
  \\
  &= \mathbb{E}_{\lambda}\!\left[
        (\, t(\beta) - \mathbb{E}_{\lambda}[t(\beta)]\,)
        (\, t(\beta) - \mathbb{E}_{\lambda}[t(\beta)]\,)^{\top}
      \right]
  \\
  &= \nabla_{\lambda}^2 a_g(\lambda)
\end{align*}
In our case, we express it wrt the natural parameters
\[
 I(\eta)
  =  \mathbb{E}_{\eta}\!\left[
        (\nabla_{\eta} \log p(\beta \mid \eta))
        (\nabla_{\eta} \log p(\beta \mid \eta))^{\top}
      \right]
\]
We can exploit the properties of the exponential family
\[ P(x | \eta) = h(x) e^{\eta^T t(x) - a(\eta)}              \]
\[ \log P(x | \eta)= \log h(x) + \eta^T t(x) - a(\eta)       \]
\[ \nabla_\eta \log P(x | \eta) = t(x) - \nabla_\eta a(\eta) \]
We use it for the definition of the Fisher Information
\[
I(\eta) = [(t(x) - \nabla_\eta a(\eta)) (t(x) - \nabla_\eta a(\eta))^T]
\]
Knowing that \( E_{p(x|\eta)}[t(x)] = \nabla a(\eta) \) , we can rewrite it as
\[
I(\eta) = [(t(x) - E_{p(x|\eta)}[t(x)]) (t(x) - E_{p(x|\eta)}[t(x)])^T]
\]
\[
I(\eta) = \nabla_\eta^2 a(\eta)
\]
In our case:
\[ a(\eta) = e^\eta \]
\[ \eta = \log \lambda \]
\[ \lambda = e^\eta \]
We get
\[
I(\eta) = e^\eta = e^{\log \lambda} = \lambda
\]
We need to use the transformation function 
\[
I_\lambda(\lambda) = I_\eta ( \eta(\lambda)) (\frac{\partial \eta}{\partial \lambda})^2
\]
\[I_\lambda(\lambda) = \lambda \cdot \frac{1}{\lambda^2} = \frac{1}{\lambda}\]
This formula can be obtained by using the chain rule in the derivation process as showed below
\[
 I(\eta)
  =  \mathbb{E}\!\left[
        (\nabla_{\eta} \log p(\beta \mid \eta))
        (\nabla_{\eta} \log p(\beta \mid \eta))^{\top}
      \right]
\]
\[
\nabla_{\eta} \log p(\beta \mid \eta) = \nabla_{\lambda} \log p(\beta \mid \lambda) \cdot \frac{d \lambda}{d \eta}
\]
\[
 I(\eta)
  =  \mathbb{E}\!\left[
        (\frac{d \lambda}{d \eta})^2
        (\nabla_{\lambda} \log p(\beta \mid \lambda))
        (\nabla_{\lambda} \log p(\beta \mid \lambda))^T
      \right]
\]
\[
I(\eta) = (\frac{d \lambda}{d \eta})^2 \mathbb{E}_\lambda\!\left[
        (\nabla_{\lambda} \log p(\beta \mid \lambda))
        (\nabla_{\lambda} \log p(\beta \mid \lambda))^T
      \right]
\]
\[
I(\eta) = (\frac{d \lambda}{d \eta})^2 \cdot I(\eta)
\]
\subsection*{Question 2.1.3}
We need just to replace each component inside the definition of exponential family
\begin{align*}
\quad & e^{(\theta_1 - 1,  - \theta_2)\binom{\log x}{x}  - \log \Gamma(\eta_1 + 1) + (\eta_1 + 1)\log(-\eta_2)} \\
%
\overset{\eta}{=} \quad & e^{(\theta_1 - 1 , - \theta_2)\binom{\log x}{x} - \log \Gamma(\theta_1 - 1 + 1) + (\theta_1 - 1 + 1)\log(-(-\theta_2|))} \\
%
= \quad & e^{(\theta_1 - 1 - \theta_2)\binom{\log x}{x} - \log \Gamma(\theta_1) + \theta_1 \log(\theta_2)} \\
%
\overset{\theta}{=} \quad & e^{(\alpha - 1 , - \beta)\binom{\log x}{x} - \log \Gamma(\alpha) + \alpha \log(\beta)} \\
%
= \quad & e^{(\alpha - 1)\log x - \beta x - \log \Gamma(\alpha) + \alpha \log(\beta)} \\
%
= \quad & \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x} \sim \text{Gamma}(\alpha, \beta)
\end{align*}

\subsection*{Question 2.1.4}
\begin{align*}
& \frac{1}{x\sqrt{2\pi}} e^{\left(\frac{\theta_1}{\theta_2}, -\frac{1}{2\theta_2}\right) \cdot \binom{\log x}{(\log x)^2} - \left(-\frac{\eta_1^2}{4\eta_2} - \frac{1}{2}\log(-2\eta_2)\right)} \\
%
\overset{\eta}{=} & \frac{1}{x\sqrt{2\pi}} e^{\left(\frac{\theta_1}{\theta_2}, -\frac{1}{2\theta_2}\right) \cdot \binom{\log x}{(\log x)^2} - \frac{\theta_1^2}{2\theta_2} + \frac{1}{2}\log \left(\frac{1}{\theta_2}\right)} \\
%
\overset{\theta}{=} & \frac{1}{x\sqrt{2\pi}} e^{\left(\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}\right) \cdot \binom{\log x}{(\log x)^2} - \frac{\mu^2}{2\sigma^2} + \frac{1}{2}\log \left(\frac{1}{\sigma^2}\right)} \\
%
= & \frac{1}{x\sqrt{2\pi\sigma^2}} e^{\left(\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}\right) \cdot \binom{\log x}{(\log x)^2} - \frac{\mu^2}{2\sigma^2}} \\
%
= & \frac{1}{x\sqrt{2\pi\sigma^2}} e^{\left(\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}\right) \cdot \binom{\log x}{(\log x)^2} - \frac{\mu^2}{2\sigma^2}} \\
%
= & \frac{1}{x\sqrt{2\pi\sigma^2}} e^{\frac{\mu}{\sigma^2}\log x - \frac{1}{2\sigma^2}(\log x)^2 - \frac{\mu^2}{2\sigma^2}} \\
%
= & \frac{1}{x\sqrt{2\pi\sigma^2}} e^{\frac{-1}{2\sigma^2}\left(-2\mu\log x + (\log x)^2 - \mu^2\right)} \\
%
= & \frac{1}{x\sqrt{2\pi\sigma^2}} e^{-\frac{(\log x - \mu)^2}{2\sigma^2}} \sim \text{ logNormal }(\mu, \sigma^2)
\end{align*}

\section*{2.3 \quad BBVI}
\subsection*{Question 2.3.8}
We know that:
\[
P(X_n | \lambda ) \sim \text{Poisson}(\lambda)
\qquad
P(\lambda) \sim \text{Gamma}(\alpha,\beta)
\]
\[
z_s \sim q(\lambda) = \text{Exponential}(\theta) 
\]
\[
p(x, \lambda) = p(\lambda) \cdot \prod_{n=1}^{N}p(x_n | \lambda ) = \left( \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda} \right) \cdot \prod_{n=1}^{N} \left( \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \right)
\]
\[
q(\lambda | \theta ) = \theta e^{-\theta \cdot \lambda}
\]
We want to write an expression for the Naive BBVI gradient estimate w.r.t \(\theta\) , starting from its general definition ( considering the paper notation , we apply the following changes \(z = \lambda\) and \( \lambda = \theta\) )
\[
\nabla_\theta\mathcal{L} =  \E_{q(\lambda|\theta)}[ \nabla_\theta \log q(\lambda | \theta) \left( \log p(x, \lambda) - \log q(\lambda | \theta) \right)]
\]
We compute noisy unbiased gradients of the ELBO with Monte Carlo samples from the variational distribution
\[
\nabla_\theta\mathcal{L} \approx \frac{1}{S} \sum_{s=1}^{S} \nabla_\theta \log q(\lambda_s | \theta) \left( \log p(x, \lambda_s) - \log q(\lambda_s | \theta) \right)
\]
with \( \lambda_s \sim q(\lambda | \theta)\)
Where:
\begin{itemize}
\item \(\log q(\lambda_s | \theta ) =  \log \theta - \theta \lambda_s\)
\item \(\nabla_\theta \log q(\lambda_s | \theta ) = \frac{1}{\theta} - \lambda_s\)
\item \( \log p( x, \lambda_s) =  \log \frac{\beta^\alpha}{\Gamma(\alpha)} + (\alpha-1)\log \lambda_s -\beta \lambda_s + \sum_{n=1}^{N} \left( x_n \log \lambda_s - \lambda_s - \log x_n! \right) \)
\end{itemize}

\subsection*{Question 2.3.9}
In the BBVI (Black Box Variational Inference) paper, Control Variates are used to reduce the variance of the gradient estimators of the ELBO while keeping them unbiased.

\section*{2.2 \quad SVI - LDA}
\subsection*{Question 2.2.5}

Given the model described in the Hoffman paper, let us define \(x_n\) as the \(n\)-th observation, \(z_n\) as the \(n\)-th local hidden variable, \(\beta\) as the global hidden variables, and \(\alpha\) as the fixed hyperparameters. According to this notation and the corresponding graphical model, the joint distribution can be written as
\begin{equation}
    p(X, Z, \beta \mid \alpha)
    = p(\beta \mid \alpha)\,
      \prod_{n=1}^{N} \textcolor{blue}{p(x_n, z_n \mid \beta)}.
\end{equation}

The equality 
\begin{equation*}
    p(x_n, z_n \mid x_{-n}, z_{-n}, \beta)
    = \textcolor{blue}{p(x_n, z_n \mid \beta)}.
\end{equation*}
holds because of the fork structure in the graphical model: given the global variables \(\beta\), each pair \((x_n, z_n)\) is conditionally independent of all other pairs \((x_{-n}, z_{-n})\), therefore it is dependent on the global variables only.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.3\linewidth]{fig/1.jpeg}
    \caption{V structure in the given model}
    \label{fig:V_structure}
\end{figure}

The global variables \(\beta\) are parameters equipped with a prior distribution \(p(\beta)\), while each local variable \(z_n\) contains the latent structure associated with the \(n\)-th observation.

The variables \(z_n\) are called \emph{local} because each of them is tied solely to its corresponding observation \(x_n\) and does not interact with the other local hidden variables.

\subsection*{Question 2.2.6}

\subsection*{Question 2.2.7}

For this question, we were asked to implement the SVI updates and the SVI algorithm for the LDA model. To do so, we followed the formulation of Latent Dirichlet Allocation given in Section 3.2 of Hoffman et al.

\begin{algorithm}[h!]
\caption{Stochastic Variational Inference for LDA}
\label{alg:SVI_LDA_exact}
\begin{algorithmic}[1]

\State Initialize $\lambda^{(0)}$ randomly.
\State Set the step-size schedule $\rho_t$ appropriately.
\Repeat
    \State Sample a document $w_d$ uniformly from the data set.
    \State Initialize $\gamma_{dk} = 1,$ for $k \in \{1,\ldots,K\}$.
    \Repeat
        \For{$n \in \{1,\ldots,N\}$}
            \State $\phi_{dn}^k \propto \exp\{\mathbb{E}[\log\theta_{dk}] + \mathbb{E}[\log\beta_{k,w_{dn}}]\}, \quad k \in \{1,\ldots,K\}$.
        \EndFor
        \State $\gamma_d = \alpha + \sum_{n} \phi_{dn}$.
    \Until local parameters $\phi_{dn}$ and $\gamma_d$ converge.
    \For{$k \in \{1,\ldots,K\}$}
        \State $\hat{\lambda}_k = \eta + D \sum_{n=1}^{N} \phi_{dn}^k w_{dn}$.
    \EndFor
    \State $\lambda^{(t)} = (1 - \rho_t)\lambda^{(t-1)} + \rho_t\hat{\lambda}$.
\Until forever

\end{algorithmic}
\end{algorithm}

\paragraph{Update $q(Z)$}
\paragraph{Update $q(\theta)$}
\paragraph{Update $q(\beta)$}



\end{document}
