\documentclass[a4paper]{scrartcl}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\elbo}{\mathcal{L}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\q}{q}

\title{Assignment 2E, 2025}
\subtitle{DD2434 Machine Learning, Advanced Course}
\author{
Riccardo Alfonso Cerrone \\
  \texttt{cerrone@kth.se}
}

\begin{document}
\maketitle

\section*{2.2 \quad SVI - LDA}
\subsection*{Question 2.2.5}

Given the model described in the Hoffman paper, let us define \(x_n\) as the \(n\)-th observation, \(z_n\) as the \(n\)-th local hidden variable, \(\beta\) as the global hidden variables, and \(\alpha\) as the fixed hyperparameters. According to this notation and the corresponding graphical model, the joint distribution can be written as
\begin{equation}
    p(X, Z, \beta \mid \alpha)
    = p(\beta \mid \alpha)\,
      \prod_{n=1}^{N} \textcolor{blue}{p(x_n, z_n \mid \beta)}.
\end{equation}

The equality 
\begin{equation*}
    p(x_n, z_n \mid x_{-n}, z_{-n}, \beta)
    = \textcolor{blue}{p(x_n, z_n \mid \beta)}.
\end{equation*}
holds because of the fork structure in the graphical model: given the global variables \(\beta\), each pair \((x_n, z_n)\) is conditionally independent of all other pairs \((x_{-n}, z_{-n})\), therefore it is dependent on the global variables only.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.3\linewidth]{fig/1.jpeg}
    \caption{V structure in the given model}
    \label{fig:V_structure}
\end{figure}

The global variables \(\beta\) are parameters equipped with a prior distribution \(p(\beta)\), while each local variable \(z_n\) contains the latent structure associated with the \(n\)-th observation.

The variables \(z_n\) are called \emph{local} because each of them is tied solely to its corresponding observation \(x_n\) and does not interact with the other local hidden variables.

\subsection*{Question 2.2.6}

\subsection*{Question 2.2.7}

For this question, we were asked to implement the SVI updates and the SVI algorithm for the LDA model. To do so, we followed the formulation of Latent Dirichlet Allocation given in Section 3.2 of Hoffman et al.

\begin{algorithm}[h!]
\caption{Stochastic Variational Inference for LDA}
\label{alg:SVI_LDA_exact}
\begin{algorithmic}[1]

\State Initialize $\lambda^{(0)}$ randomly.
\State Set the step-size schedule $\rho_t$ appropriately.
\Repeat
    \State Sample a document $w_d$ uniformly from the data set.
    \State Initialize $\gamma_{dk} = 1,$ for $k \in \{1,\ldots,K\}$.
    \Repeat
        \For{$n \in \{1,\ldots,N\}$}
            \State $\phi_{dn}^k \propto \exp\{\mathbb{E}[\log\theta_{dk}] + \mathbb{E}[\log\beta_{k,w_{dn}}]\}, \quad k \in \{1,\ldots,K\}$.
        \EndFor
        \State $\gamma_d = \alpha + \sum_{n} \phi_{dn}$.
    \Until local parameters $\phi_{dn}$ and $\gamma_d$ converge.
    \For{$k \in \{1,\ldots,K\}$}
        \State $\hat{\lambda}_k = \eta + D \sum_{n=1}^{N} \phi_{dn}^k w_{dn}$.
    \EndFor
    \State $\lambda^{(t)} = (1 - \rho_t)\lambda^{(t-1)} + \rho_t\hat{\lambda}$.
\Until forever

\end{algorithmic}
\end{algorithm}

\paragraph{Update $q(Z)$}
\paragraph{Update $q(\theta)$}
\paragraph{Update $q(\beta)$}



\end{document}
